import os
import time
import logging
import warnings
def enable_quiet_mode(on=True):
    if not on:
        return
    warnings.filterwarnings('ignore', message='The NumPy global RNG was seeded.*', category=FutureWarning)
    warnings.filterwarnings('ignore', message='The total space of parameters .* smaller than n_iter', category=UserWarning, module=r'sklearn\.model_selection\._search')
    warnings.filterwarnings('ignore', message='Degrees of freedom <= 0 for slice', category=RuntimeWarning, module=r'numpy\.core\._methods')
    warnings.filterwarnings('ignore', message='invalid value encountered in scalar divide', category=RuntimeWarning, module=r'numpy\.core\._methods')
import sys
assert sys.version_info >= (3,10), "This script requires Python 3.10+"
import json
import re
from glob import glob
from typing import List, Tuple, Dict, Any, Optional
import xlsxwriter

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import sklearn
from sklearn.base import BaseEstimator, clone
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import TimeSeriesSplit
from sklearn.calibration import calibration_curve
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import (
    cohen_kappa_score, roc_auc_score, average_precision_score, ConfusionMatrixDisplay, accuracy_score)
from sklearn.feature_selection import RFE
from tqdm import tqdm

import xgboost as xgb

import warnings
warnings.filterwarnings("ignore", message="Degrees of freedom <= 0 for slice", category=RuntimeWarning)
warnings.filterwarnings("ignore", message="invalid value encountered in scalar divide", category=RuntimeWarning)

# --- Optional Dependencies ---
try:
    import mord
    MORD_AVAILABLE = True
except ImportError:
    mord = None
    MORD_AVAILABLE = False
    logging.warning("mord not found. The Ordinal Logistic Regression model will be skipped.")

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    shap = None
    SHAP_AVAILABLE = False
    logging.warning("shap not found. SHAP analysis will be skipped.")


# ============================== CONSTANTS & CONFIGURATION =======================================

# --- Cleaned Column Names ---
COL_ID = 'ODIN_ID'; COL_TRAUMA_DATETIME = 'Start_of_Disease'; COL_TRAUMA_YEAR = 'Trauma_Year'
COL_TRAUMA_UNCERTAINTY = 'Range_of_uncertainty_of_trauma_time(hrs)'; COL_ICU_TIME = 'Time'
COL_OUTCOME = 'Outcome - Extended Glasgow Outcome Scale (uppföljning)'
COL_GOSE_FOLLOWUP_MONTHS = "Number of months from registration date (follow-up)"
COL_DC_TYPE = 'DC_type'; COL_DC_TIME = 'Time_from_TBI_to_DC_in_days'; COL_GCS_EYE_TEXT = 'Eye Response'
COL_GCS_VERBAL_TEXT = 'Verbal Response'; COL_GCS_MOTOR_TEXT = 'Motor Response'
COL_PUPIL_L_REACT = 'Left pupil reaction'; COL_PUPIL_R_REACT = 'Right pupil reaction'
COL_PUPIL_L_SIZE = 'PUPIL_LEFT_SIZE'; COL_PUPIL_R_SIZE = 'PUPIL_RIGHT_SIZE'
COL_MARSHALL_SCORE = 'Marshall'; COL_DATE_OF_DEATH = 'Date_of_death'
COL_CT_FINDINGS = 'CT-fynd vid ankomst'

# --- Derived Column Names ---
COL_TARGET_GOSE_1_8 = 'GOSE_1_8'; COL_TARGET_ORDINAL_0_6 = 'GOSE_Ordinal_0_6'
COL_TARGET_MORTALITY = 'Mortality'; COL_TARGET_FAVORABLE = 'FavorableOutcome'
COL_TIME_AFTER_TRAUMA = 'time_from_trauma'; COL_PUPIL_REACT_COMBINED = 'Pupil_Reactivity_Combined'
COL_GCS_EYE_SCORE = 'Eye_Score'; COL_GCS_VERBAL_SCORE = 'Verbal_Score'; COL_GCS_MOTOR_SCORE = 'Motor_Score'; COL_GCS_SUM = 'GCS_Sum'

# --- Model Names ---
MODEL_XGB = "XGBoost"; MODEL_ORDINAL = "OrdinalLogistic"

# --- Main Configuration Dictionary ---
CONFIG = {
    "RUN_INFO": {"RUN_ID": time.strftime("%Y%m%d_%H%M%S"), "OUTPUT_FOLDER": f"./output/GOSE_Prediction_v11.0_{time.strftime('%Y%m%d_%H%M%S')}/"},
    "FILE_PATHS": {"MASTER_FILE_PATH": "./data/RawICUData/master_file.csv", "ICU_DATA_FOLDER": "./data/RawICUData/"},
    "DATA_PROCESSING": {
        "TEST_SET_FRACTION": 0.2,
        "SPLIT_STRATEGY": "stratified", #stratified or temporal (also change cv_type in MODELING). Set class_weights='none' for temporal split and balanced for stratified.
        "GOSE_FOLLOWUP_MAX_MONTHS": 12,
        "MAX_TRAUMA_UNCERTAINTY_HRS": 6
    },
    "FEATURE_ENGINEERING": {
        "PREDICTION_LANDMARKS_HOURS": [24, 48],
        "DYNAMIC_VARS": ["ICPm", "CPP", "BPm", "PRx"],
        "ICP_AUC_THRESHOLDS": [10, 20],
        "EVENT_BURDEN_VARS": {
            'icpm': {'above': [14, 20, 22, 35], 'below': [10]},
            'cpp': {'below': [60, 50]},
            'bpm': {'above': [120]}
        },
        "BEGIN_WINDOW_HOURS": 24, "END_WINDOW_HOURS": 8,
        "PLAUSIBLE_RANGES": {
            "ICPm": (-5, 100), "BPm": (20, 160), "CPP": (0, 150), "PRx": (-1, 1),
        },
        "MAX_FFILL_MINUTES": 15, "MIN_DATA_POINTS_PER_VAR": 5, "FFILL_FOR_EPISODES_MINUTES": 3,
        "EVENT_SIGNATURE": {
            "ICP_GT_MAIN": 20.0,
            "RECOVERY_TO_ICP": 20.0,
            "CPP_LT_MAIN": 50.0,
            "PRX_GT_MAIN": 0.3,
            "JOINT_EVENT_ICP_THRESHOLDS": [20.0, 10.0, 5.0],
            "MIN_EPISODE_MINUTES": 5,
            "MERGE_GAP_MINUTES": 2,
            "RECOVERY_STABILITY_MIN": 5,
            "ICP_PEAK_BINS": [20.0, 25.0, 35.0],
            "DURATION_BINS_MIN": [5, 15, 35, 60],
}
    },
    "MODELING": {
        "RANDOM_STATE": 1, "CV_SPLITS": 10, "HYPERPARAM_N_ITER": 50, "N_JOBS": -1, #(CV splits 10, hyperparam 50)
        "THRESHOLD_MODE": "hybrid",   # one of: "qwk", "prior", "hybrid".
        "PRIOR_WEIGHT": 0.25,          # how much to pull towards prior-matched cutpoints
        "CLASS_WEIGHTS": 'balanced', # Suggest set class_weights='none' for temporal split and balanced for stratified. Consider class specific {0: 1.5, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.25, 6: 2.0}. 
        "CLASS_WEIGHT_EXPONENT": 0.5, # only used if CLASS_WEIGHTS='balanced', suggest try 0.0-1.0
        "CV_TYPE": "stratified", # 'stratified' or 'timeseries' (also change split_strategy in DATA_PROCESSING)
        "PARAM_GRIDS": {
            MODEL_XGB: {'classifier__n_estimators': [400, 600, 800, 1000], 'classifier__max_depth': [1, 3, 4, 6, 8],
                'classifier__learning_rate': [0.03, 0.06, 0.1], 'classifier__subsample': [0.5, 0.7, 1.0],
                'classifier__colsample_bytree': [0.5, 0.6, 1.0], 'classifier__gamma': [0.5, 1.0, 2.0, 3.0],
                'classifier__reg_lambda': [0.5, 1.5, 2.0, 3.0], 'classifier__reg_alpha': [0, 0.25, 0.5, 1.0],
                "classifier__min_child_weight": [1, 3, 5, 10, 30]
            },
            MODEL_ORDINAL: {
                'classifier__alpha': np.logspace(-3, 2, 6)
            }
        }
    },
    "ARTIFACTS": {"SHAP_SAMPLE_SIZE": 1000, "BOOTSTRAP_N": 1000, "GOSE_ORDINAL_LABELS": {0: "Dead/VS", 1: "Sev-L", 2: "Sev-H", 3: "Mod-L", 4: "Mod-H", 5: "Good-L", 6: "Good-H"}},
    "FEATURE_SELECTION": {"ENABLE_ICU_SELECTION": True, "TOP_N_ICU_FEATURES": 75, "RFE_STEP": 0.1} # Select the 75 most important ICU features

}

# --- Feature Lists ---

USER_PROVIDED_FEATURES = {
    "numeric_baseline": [
        "Age",
        COL_GCS_EYE_SCORE,
        COL_GCS_SUM,
        COL_GCS_MOTOR_SCORE,
        COL_PUPIL_L_SIZE,
        COL_PUPIL_R_SIZE,
        COL_GCS_VERBAL_SCORE,
        COL_DC_TIME,
        COL_MARSHALL_SCORE
    ],
    "categorical_baseline": [
        COL_CT_FINDINGS,
        COL_DC_TYPE,
        "Fraktur öppen eller sluten",
        "Gender",
        "Mechanism of injury",
        "RLS",
        "Referring hospital",
        "Type of ischemia",
        "Paresertyp",
        "Type of previous brain injury/illness",
        "Vehicle",
        COL_PUPIL_REACT_COMBINED,
        "Driver",
        COL_GCS_EYE_TEXT,
        COL_GCS_VERBAL_TEXT,
        COL_GCS_MOTOR_TEXT
    ],
    "binary_baseline": [
        "Buk",
        "Alcoholism",
        "Anticoagulants",
        "Barbiturater",
        "CT-bilder granskade",
        "DC_local_hospital",
        "Diabetes",
        "Emergency surgery at local hospital",
        "Omfattande blödning",
        "Extremiteter",
        "Ansikte",
        "Hypertension/Cardiovascular disease",
        "Hypothermia associated with the accident",
        "Influence of alcohol/drugs at the time of accident",
        "Andra skador bedömt",
        "Pareser",
        "Bäcken",
        "Previous brain injury/illness",
        "Severe global ischemia after the accident",
        "Ryggmärgsskada",
        "Kotpelarskada",
        "Thorax",
        "Was the patient transported by air ambulance?",
        "Intrakraniell_hematom-utrymning",
        COL_PUPIL_L_REACT,
        COL_PUPIL_R_REACT
    ]
}

GOSE_TEXT_TO_NUMERIC_MAP = {
    "dead": 1, "vegetative state": 2, "vs": 2,
    "severe disability lower sd-l": 3, "sd-l": 3,
    "severe disability higher sd-h": 4, "sd-h": 4,
    "moderate disability lower md-l": 5, "md-l": 5,
    "moderate disability higher md-h": 6, "md-h": 6,
    "good recovery lower gr-l": 7, "gr-l": 7,
    "good recovery higher gr-h": 8, "gr-h": 8,
}

GOSE_1_8_TO_ORDINAL_0_6_MAP = {1: 0, 2: 0, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6}

# ============================== SETUP & HELPERS =======================================
# This function prepares the workspace for the analysis. It creates a new folder to save all results,
# sets up a log file to record every step the program takes, and saves the settings used for this specific run.
def setup_environment(cfg: Dict):
    os.makedirs(cfg["RUN_INFO"]["OUTPUT_FOLDER"], exist_ok=True)
    log_file = os.path.join(cfg["RUN_INFO"]["OUTPUT_FOLDER"], "pipeline.log")

    # Force reconfigure root logger (works on Python 3.8+)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s",
                        handlers=[], force=True)

    # Build handlers explicitly
    fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    fh = logging.FileHandler(log_file, mode="w", encoding="utf-8")
    fh.setFormatter(fmt)
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)

    root = logging.getLogger()
    root.addHandler(fh)
    root.addHandler(sh)

    # Pipe Python warnings into logging (so those big sklearn messages land in the file)
    logging.captureWarnings(True)
    warnings.simplefilter("default")

    logging.info(f"--- Starting Pipeline --- Run ID: {cfg['RUN_INFO']['RUN_ID']} ---")
    logging.info("Artifacts → %s", os.path.abspath(cfg["RUN_INFO"]["OUTPUT_FOLDER"]))
    with open(os.path.join(cfg["RUN_INFO"]["OUTPUT_FOLDER"], "config.json"), "w", encoding="utf-8") as f:
        json.dump(json.loads(json.dumps(cfg, default=str)), f, indent=4)

# This function cleans up text, like column headers from a data table. It removes spaces,
# special characters, and ensures the name doesn't start with a number, making it safe for the program to use.
def sanitize_name(name: str) -> str:
    return re.sub(r'\W|^(?=\d)', '_', str(name))


# This is a statistical method to assess the reliability of a performance metric (like AUC).
# It works by repeatedly creating new, slightly different patient groups from the original data
# and calculating the metric for each. This process generates a 95% confidence interval, giving us
# a range of plausible values for the metric, not just a single number.
def bootstrap_metric(y_true, y_pred, metric_func, n_bootstraps=1000, random_state=42, **kwargs):
    rng = np.random.RandomState(random_state); n = len(y_true); bootstrapped_scores = []
    for _ in range(n_bootstraps):
        indices = rng.choice(np.arange(n), size=n, replace=True)
        if len(np.unique(y_true[indices])) < 2 and metric_func in (roc_auc_score, average_precision_score): continue
        if len(indices) > 0:
            bootstrapped_scores.append(metric_func(y_true[indices], y_pred[indices], **kwargs))
    if not bootstrapped_scores: return metric_func(y_true, y_pred, **kwargs), np.nan, np.nan
    point_estimate = metric_func(y_true, y_pred, **kwargs)
    lower = np.percentile(bootstrapped_scores, 2.5); upper = np.percentile(bootstrapped_scores, 97.5)
    return point_estimate, lower, upper


# This function calculates how often the model's predictions are within N categories of the true outcome.
def accuracy_within_n(y_true, y_pred, n=1):
    """Calculates accuracy within N categories."""
    return np.mean(np.abs(np.asarray(y_true) - np.asarray(y_pred)) <= n)

# This function reads a data table (like a CSV file) from the ICU files. It's built to be flexible,
# automatically trying to figure out how the data is separated (e.g., by commas or semicolons)
# and to avoid crashing if it encounters a file it can't read.
def read_icu_table(path: str) -> pd.DataFrame | None:
    try:
        df = pd.read_csv(path, sep='[,;]', engine='python', encoding_errors='ignore')
        if df.shape[1] > 1 and len(df) > 0: return df
    except Exception: pass
    try:
        df = pd.read_csv(path, sep=None, engine='python', encoding_errors='ignore')
        if df.shape[1] > 1 and len(df) > 0: return df
    except Exception as e:
        logging.warning(f"[ICU] Failed to parse {os.path.basename(path)}. Error: {e}")
        return None

def _try_parse_with_formats(s: pd.Series) -> pd.Series:
    """Try a few common datetime formats fast; fall back quietly to coerce+cache."""
    common = [
        "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M",
        "%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M",
        "%m/%d/%Y %H:%M:%S", "%m/%d/%Y %H:%M"
    ]
    s = s.astype(str)
    for fmt in common:
        try:
            return pd.to_datetime(s, format=fmt, errors="raise")
        except Exception:
            continue
    return pd.to_datetime(s, errors="coerce", cache=True)

# This function makes sure that each ICU data table has a proper, standardized timestamp column.
# It intelligently searches for columns that likely represent time (e.g., 'time', 'clock',
# or a combination of 'date' and 'tid') and converts them into a consistent format that the
# program can use for time-based analysis.
def ensure_icu_timestamp(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure a standardized datetime column for ICU records.

    Priority:
      1) Use the sanitized ICU time column if it exists.
      2) Else, try to build from a likely time column (e.g., 'time', 'tid', 'timestamp', 'clock')
         and, if available, a separate date column (containing 'date').
    Result is stored in the sanitized name of COL_ICU_TIME.
    """
    s_icu_time = sanitize_name(COL_ICU_TIME)

    # Case 1: standardized ICU time column already present
    if s_icu_time in df.columns:
        # Only parse if not already datetime64
        if not np.issubdtype(df[s_icu_time].dtype, np.datetime64):
            df[s_icu_time] = _try_parse_with_formats(df[s_icu_time])
        return df

    # Case 2: infer from other columns
    # Look for a time-like column
    time_candidates = [c for c in df.columns
                       if any(k in c.lower() for k in ['time', 'tid', 'timestamp', 'clock'])]
    if time_candidates:
        dt_col = time_candidates[0]

        # If a separate date-like column exists, combine "date + time"
        date_candidates = [c for c in df.columns
                           if ('date' in c.lower()) and (c != dt_col)]
        if date_candidates:
            combined = df[date_candidates[0]].astype(str).str.strip() + ' ' + df[dt_col].astype(str).str.strip()
            df[s_icu_time] = _try_parse_with_formats(combined)
        else:
            df[s_icu_time] = _try_parse_with_formats(df[dt_col])

    # If no candidates found, or parsing failed, caller will handle downstream
    return df

# This function cleans the ICU data by removing clinically implausible values. For example,
# it can be set to remove a heart rate of 500 or a blood pressure of 0. It replaces these
# out-of-range values with a 'missing' marker, so they don't corrupt the analysis.
def filter_plausible_icu_values(df_icu: pd.DataFrame, value_filters: Dict[str, Tuple[float, float]]) -> pd.DataFrame:
    df_filtered = df_icu.copy()
    for col, (min_val, max_val) in value_filters.items():
        s_col = sanitize_name(col)
        if s_col in df_filtered.columns:
            numeric_col = pd.to_numeric(df_filtered[s_col], errors='coerce')
            mask = (numeric_col < min_val) | (numeric_col > max_val)
            num_removed = mask.sum()
            if num_removed > 0:
                df_filtered.loc[mask, s_col] = np.nan
                logging.debug(f"[Filter] Removed {num_removed} implausible values from '{s_col}' (range: {min_val}-{max_val}).")
    return df_filtered

# This function handles short gaps in time-series data (like vital signs). If a measurement is missing,
# it 'carries forward' the last known value, but only for a limited time (e.g., for up to 30 minutes).
# This prevents us from assuming a value is stable for too long when we don't have a new measurement.
def local_forward_fill(df: pd.DataFrame, time_col: str, data_cols: List[str], max_ffill_minutes: int) -> pd.DataFrame:
    if df.empty or max_ffill_minutes <= 0: return df
    df_sorted = df.set_index(time_col).sort_index()
    df_ffilled = df_sorted[data_cols].resample('1min').ffill(limit=max_ffill_minutes)
    df_reindexed = df_ffilled.reindex(df_sorted.index)
    return df_reindexed.reset_index()

# This function removes variables (columns) from the dataset that have too much missing data.
# For example, if a lab test was only performed on 10% of patients, this function would remove
# that variable because it's not informative enough for the model.
def drop_sparse_columns(df: pd.DataFrame, min_frac: float = 0.25) -> pd.DataFrame:
    if df.empty: return df
    thresh = int(np.ceil(min_frac * len(df)))
    counts = df.notna().sum()
    keep = counts[counts >= thresh].index
    drop = counts[counts <  thresh].index
    if len(drop):
        logging.info(f"Dropping {len(drop)} sparse columns (<{min_frac:.0%} coverage): {list(drop)[:12]}...")
    return df[keep]

# This function performs a 'clinical sanity check' on the processed data before building the model.
# It compares the average values of key features (like ICP) between patients with favorable outcomes
# and those with poor outcomes. This helps confirm that our data processing hasn't introduced errors
# and that the data reflects expected clinical patterns (e.g., that average ICP is generally
# higher in patients who do poorly).
def perform_sanity_check_on_features(
    dynamic_features_df: pd.DataFrame,
    y_data: pd.DataFrame,
    features_to_check: List[str],
    outcome_col: str = COL_TARGET_FAVORABLE,
    group_name: str = "Training Set"
):
    """
    Performs an automated sanity check by comparing summary stats of key features
    between good and poor outcome groups.
    """
    logging.info(f"\n--- Automated Sanity Check for Dynamic Features ({group_name}) ---")

    if dynamic_features_df.empty:
        logging.warning("  Sanity Check: Dynamic features DataFrame is empty. Skipping.")
        return

    # Align features with outcomes
    combined_df = dynamic_features_df.join(y_data[outcome_col], how="inner")

    if combined_df.empty:
        logging.warning("  Sanity Check: No overlapping patients between features and outcomes. Skipping.")
        return

    combined_df[outcome_col] = combined_df[outcome_col].map({0: 'Poor_Outcome', 1: 'Favorable_Outcome'})

    # Filter to only the features we want to inspect
    features_exist = [f for f in features_to_check if f in combined_df.columns]
    if not features_exist:
        logging.warning("  Sanity Check: None of the specified features to check were found. Skipping.")
        return

    # Use groupby to calculate statistics for each group
    summary = combined_df.groupby(outcome_col)[features_exist].agg(['mean', 'std', 'median', 'count'])

    # Transpose for better readability
    summary = summary.T

    logging.info(f"Comparing key features between Favorable vs. Poor outcome groups:\n"
                 f"{summary.to_string(float_format='%.2f')}\n")

    # Example check: Mean ICP should be higher in the poor outcome group
    icp_mean_feat = 'dyn_ICPm_mean'
    if icp_mean_feat in features_exist and 'Poor_Outcome' in summary.columns and 'Favorable_Outcome' in summary.columns:
        mean_icp_poor = summary.loc[(icp_mean_feat, 'mean'), 'Poor_Outcome']
        mean_icp_good = summary.loc[(icp_mean_feat, 'mean'), 'Favorable_Outcome']
        if pd.notna(mean_icp_poor) and pd.notna(mean_icp_good):
            if mean_icp_poor > mean_icp_good:
                logging.info(f"  [PASS] Mean ICP is higher in Poor Outcome group ({mean_icp_poor:.2f}) than Favorable ({mean_icp_good:.2f}).")
            else:
                logging.warning(f"  [FAIL] Mean ICP is NOT higher in Poor Outcome group ({mean_icp_poor:.2f}) than Favorable ({mean_icp_good:.2f}). Check data processing.")

def _minute_grid(landmark_h: int) -> pd.TimedeltaIndex:
    """Return a 1-min TimedeltaIndex from 0:00 up to and INCLUDING H:00.
    Length = H*60 + 1 minutes.
    """
    return pd.timedelta_range(start="0s", end=f"{int(landmark_h)}h", freq="1min")

def _grid_align_generic(s: pd.Series, grid_index: pd.TimedeltaIndex, ffill_minutes: int) -> pd.Series:
    """Align an irregular Timedelta-indexed series onto a 1-min grid with limited ffill."""
    s = pd.to_numeric(s, errors='coerce').dropna()
    if s.empty:
        return pd.Series(index=grid_index, dtype=float)
    s = s.sort_index()
    g = s.resample('1min').ffill(limit=ffill_minutes)
    return g.reindex(grid_index)

def _grid_align_noffill(s: pd.Series, grid_index: pd.TimedeltaIndex) -> pd.Series:
    """Align to the grid with **no** forward fill (true observation mask)."""
    s = pd.to_numeric(s, errors='coerce').dropna()
    if s.empty:
        return pd.Series(index=grid_index, dtype=float)
    s = s.sort_index()
    g = s.resample('1min').asfreq() # no fill
    return g.reindex(grid_index)


def _decode_with_thresholds(proba: np.ndarray, thresholds: np.ndarray) -> np.ndarray:
    """Ordinal decode: y_hat = sum_k [S_k >= tau_k], where S_k = P(Y>=k), k=1..6.
    proba shape: (n, 7) for classes 0..6. thresholds shape: (6,).
    """
    # cumulative from the **top** class downward, then drop the final zeros column
    S = np.cumsum(proba[:, ::-1], axis=1)[:, :-1] # shape (n, 6) for k=1..6
    # enforce nonincreasing thresholds just in case
    tau = np.minimum.accumulate(thresholds)
    # compare each column to its tau, then sum across k
    return (S >= tau.reshape(1, -1)).sum(axis=1)

def fit_prior_matched_cum_thresholds(pipeline: Pipeline, X: pd.DataFrame, y: pd.Series,
                                     tau_clip: Tuple[float,float]=(0.02, 0.98)) -> Optional[np.ndarray]:
    """
    Choose cumulative cutpoints tau_k so that predicted class proportions
    roughly match the training priors P(Y>=k). Works with any classifier
    that exposes predict_proba for classes 0..6.

    Returns tau of shape (6,) with tau_1 >= tau_2 >= ... >= tau_6.
    """
    clf = pipeline.named_steps.get('classifier', None)
    if clf is None or not hasattr(clf, 'predict_proba'):
        return None

    y_true = y.astype(int).values
    proba  = pipeline.predict_proba(X)

    # cumulative S_k = P(Y>=k) for k=1..6
    S = np.cumsum(proba[:, ::-1], axis=1)[:, :-1]   # (n, 6)
    # training cumulative priors p_k = P(Y>=k)
    n = y_true.size
    pks = np.array([(y_true >= k).mean() for k in range(1, 7)], dtype=float)  # (6,)

    taus = []
    for k in range(6):
        # we want fraction of samples with S_k >= tau_k ≈ p_k,
        # so tau_k is the (1 - p_k) quantile of S_k
        q = float(np.clip(1.0 - pks[k], 0.0, 1.0))
        val = np.quantile(S[:, k], q)
        taus.append(val)

    tau = np.array(taus, dtype=float)
    # clip to a sane range and enforce nonincreasing tau_1 >= ... >= tau_6
    lo, hi = tau_clip
    tau = np.clip(tau, lo, hi)
    tau = np.minimum.accumulate(tau)  # monotone
    return tau

def fit_qwk_cum_thresholds(pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, grid_points: int = 21, tau_low: float = 0.05, tau_high: float = 0.95) -> Optional[np.ndarray]:
    """Learn monotone cumulative-probability thresholds that maximize QWK on the TRAINING set.
    Notes:
    - This is a light-weight post-fit calibration of decision cutpoints (not probabilities).
    - For best rigor you can switch to out-of-fold predictions later; in-sample works well
    in practice because only 6 parameters are tuned.
    """
    clf = pipeline.named_steps.get('classifier', None)
    if clf is None or not hasattr(clf, 'predict_proba'):
        return None
    y_true = y.astype(int).values
    proba = pipeline.predict_proba(X)
    # Build cumulative S_k = P(Y>=k)
    S = np.cumsum(proba[:, ::-1], axis=1)[:, :-1] # (n, 6)
    # Greedy coordinate search per k with a small monotonicity constraint
    grid = np.linspace(tau_low, tau_high, grid_points) # reasonable band for cutpoints
    thresholds = np.full(6, 0.5, dtype=float)
    prev_tau = tau_high # start from the top, enforce nonincreasing tau
    for k in range(6):
        best_tau = thresholds[k]
        best_qwk = -1.0
        for tau in grid:
            if tau > prev_tau:
                continue # enforce nonincreasing tau_1>=tau_2>=...>=tau_6
            tmp = thresholds.copy()
            tmp[k] = tau
            y_hat = _decode_with_thresholds(proba, tmp)
            qwk = cohen_kappa_score(y_true, y_hat, weights='quadratic')
            if qwk > best_qwk:
                best_qwk, best_tau = qwk, tau
        thresholds[k] = best_tau
        prev_tau = best_tau
    return thresholds

def predict_with_qwk_thresholds(pipeline: Pipeline, X: pd.DataFrame) -> np.ndarray:
    """Predict ordinal classes using stored cumulative thresholds when available.
    Falls back to pipeline.predict(). Also normalizes mord outputs (1..7 -> 0..6).
    """
    clf = pipeline.named_steps.get('classifier', None)
    if clf is not None and hasattr(clf, 'predict_proba') and hasattr(pipeline, 'qwk_thresholds_'):
        try:
            proba = pipeline.predict_proba(X)
            y_hat = _decode_with_thresholds(proba, pipeline.qwk_thresholds_)
            return y_hat.astype(int)
        except Exception:
            pass # fall back below if anything odd happens
    y_pred = pipeline.predict(X)
    # mord models output 1..7 → shift to 0..6
    try:
        if np.issubdtype(y_pred.dtype, np.integer) and y_pred.min() == 1 and y_pred.max() == 7:
            y_pred = y_pred - 1
    except Exception:
        pass
    return y_pred.astype(int)

def _qwk_scorer_basic(estimator: Pipeline, X: pd.DataFrame, y: pd.Series) -> float:
    """Custom scorer for RandomizedSearchCV: compute QWK on estimator.predict().
    We do not use thresholds here to keep CV inner-loop stable & fast.
    """
    y_true = y.astype(int).values
    y_pred = estimator.predict(X)
    # mord safeguard
    try:
        if np.issubdtype(y_pred.dtype, np.integer) and y_pred.min() == 1 and y_pred.max() == 7:
            y_pred = y_pred - 1
    except Exception:
        pass
    return cohen_kappa_score(y_true, y_pred, weights='quadratic')

def _calibration_slope_ci(
    y_true, y_prob, alpha: float = 0.05, method: str = "auto",
    n_boot: int = 2000, random_state: int = 0, robust: bool = False
) -> Tuple[float, float, float, float, str]:
    """Fit logit(y) = a + b*logit(p_hat) and return (slope b, lo, hi, intercept a, method_used)."""
    y = np.asarray(y_true, dtype=int)
    p = np.asarray(y_prob, dtype=float)
    eps = 1e-15
    p = np.clip(p, eps, 1 - eps)
    x = np.log(p / (1 - p))

    if method in ("auto", "wald"):
        try:
            import statsmodels.api as sm
            X = sm.add_constant(x)
            fit = sm.Logit(y, X).fit(disp=False)
            if robust:
                fit = fit.get_robustcov_results(cov_type="HC3")
            slope = float(fit.params[1])
            se = float(fit.bse[1])
            z = 1.959963984540054
            lo, hi = slope - z*se, slope + z*se
            intercept = float(fit.params[0])
            return slope, lo, hi, intercept, "wald"
        except Exception as e:
            if method == "wald":
                raise
            logging.info(f"Statsmodels/Wald failed; falling back to bootstrap: {e}")

    # Bootstrap percentile CI
    from sklearn.linear_model import LogisticRegression
    rng = np.random.default_rng(random_state)

    def _fit(xb, yb):
        try:
            lr = LogisticRegression(penalty="none", solver="lbfgs", max_iter=2000)
        except Exception:
            lr = LogisticRegression(C=1e6, solver="lbfgs", max_iter=2000)
        lr.fit(xb.reshape(-1,1), yb)
        return float(lr.coef_[0][0]), float(lr.intercept_[0])

    slope_hat, intercept_hat = _fit(x, y)
    slopes = []
    for _ in range(int(n_boot)):
        idx = rng.integers(0, len(y), len(y))
        sb, _ = _fit(x[idx], y[idx])
        slopes.append(sb)
    lo, hi = np.percentile(slopes, [100*(alpha/2), 100*(1-alpha/2)])
    return slope_hat, float(lo), float(hi), intercept_hat, "bootstrap"

def _wilson_ci(k: int, n: int, alpha: float = 0.05) -> Tuple[float, float]:
    if n == 0:
        return (np.nan, np.nan)
    from math import sqrt
    z = 1.959963984540054
    p = k / n
    denom = 1 + z**2/n
    centre = (p + z**2/(2*n)) / denom
    half = z * sqrt((p*(1-p) + z**2/(4*n))/n) / denom
    return max(0.0, centre-half), min(1.0, centre+half)

def _reliability_table(y_true, y_prob, n_bins: int = 20, strategy: str = "quantile") -> pd.DataFrame:
    y = np.asarray(y_true).astype(int)
    p = np.asarray(y_prob).astype(float)
    m = np.isfinite(y) & np.isfinite(p)
    y, p = y[m], p[m]
    if y.size == 0:
        return pd.DataFrame(columns=["p_mean","obs_rate","n","ci_lo","ci_hi"])

    if strategy == "quantile":
        qs = np.linspace(0,1,n_bins+1)
        edges = np.quantile(p, qs)
        edges = np.unique(np.clip(edges, 0.0, 1.0))
        if edges.size < 3:
            edges = np.linspace(0,1,min(n_bins, max(2, len(np.unique(p))-1))+1)
    else:
        edges = np.linspace(0,1,n_bins+1)

    idx = np.digitize(p, edges, right=False) - 1
    idx = np.clip(idx, 0, edges.size-2)

    rows = []
    for b in range(edges.size-1):
        mb = idx == b
        n = int(mb.sum())
        if n == 0:
            continue
        p_mean = float(p[mb].mean())
        k = int(y[mb].sum())
        obs = k/n
        lo, hi = _wilson_ci(k, n)
        rows.append({"p_mean":p_mean, "obs_rate":obs, "n":n, "ci_lo":lo, "ci_hi":hi})
    return pd.DataFrame(rows).sort_values("p_mean").reset_index(drop=True)

def _plot_cal_panel(
    ax, y_true, y_prob, title: str,
    n_bins: int = 20, strategy: str = "quantile",
    slope_kwargs: Optional[dict] = None
):
    tab = _reliability_table(y_true, y_prob, n_bins=n_bins, strategy=strategy)
    ax.plot([0,1],[0,1], linestyle="--", linewidth=1)
    ax.plot(tab["p_mean"], tab["obs_rate"], linewidth=2)
    ax.fill_between(tab["p_mean"], tab["ci_lo"], tab["ci_hi"], alpha=0.2)

    ax2 = ax.twinx()
    ax2.hist(np.clip(y_prob, 0, 1), bins=40, range=(0,1), density=True, alpha=0.12)
    ax2.set_yticks([]); ax2.set_ylim(0, None)

    txt = "Slope: n/a"
    try:
        slope_kwargs = slope_kwargs or {}
        s, lo, hi, itc, how = _calibration_slope_ci(y_true, y_prob, **slope_kwargs)
        txt = f"Slope {s:.2f} (95% {lo:.2f}–{hi:.2f})\nIntercept {itc:.2f}\nCI via {how}"
    except Exception as e:
        logging.warning(f"Calibration slope failed: {e}")

    ax.text(0.03, 0.97, txt, transform=ax.transAxes, va="top", ha="left", fontsize=9,
            bbox=dict(boxstyle="round,pad=0.25", fc="white", alpha=0.85, ec="none"))
    ax.set_title(title); ax.set_xlim(0,1); ax.set_ylim(0,1)
    ax.set_xlabel("Predicted probability"); ax.set_ylabel("Observed probability")
    return tab

# ============================== DATA LOADING & PREPARATION =================================
# This function reads the main patient information file (the 'master file') which contains baseline
# data like demographics, injury details, and outcomes. It then performs a series of critical cleaning
# and preparation steps to make the data consistent and ready for analysis.

def load_and_prepare_master_data(cfg: Dict) -> Tuple[Tuple[pd.DataFrame, pd.DataFrame], List[str], List[Tuple[str, int]]]:
    """
    Loads and preprocesses the master file.
    
    This function performs initial data cleaning, outcome mapping, and GCS filtering.
    Crucially, it performs the main train/test split. For the analysis to be valid,
    the split is performed ONLY on the subset of patients who have a valid trauma time,
    as this is a prerequisite for the time-dependent landmark analysis.
    """
    logging.info("--- 1. Loading and Preparing Master Data ---")

    flow_log = []

    df_master = pd.read_csv(
        cfg["FILE_PATHS"]["MASTER_FILE_PATH"],
        sep=None, engine='python', encoding_errors='ignore'
    )
    sanitized_map = {orig: sanitize_name(orig) for orig in df_master.columns}
    df_master.columns = df_master.columns.map(sanitized_map)

    flow_log.append(("Total patients in raw master file", len(df_master)))

    def log_step(df_before, df_after, reason, cumulative_label=""):
        flow_log.append((f"  — Excluded: {reason}", len(df_before) - len(df_after)))
        if cumulative_label:
            flow_log.append((cumulative_label, len(df_after)))
        return df_after

    # --- Duplicate Removal ---
    s_id = sanitize_name(COL_ID)
    df_before_dupes = df_master.copy()
    df_master[s_id] = df_master[s_id].astype(str).str.strip()
    df_master.drop_duplicates(subset=[s_id], inplace=True)
    df_master = log_step(df_before_dupes, df_master, "Duplicate patient records", "Patients after duplicate removal")

    # --- GCS Processing and Filtering ---
    s_eye_txt, s_verbal_txt, s_motor_txt = map(sanitize_name, [COL_GCS_EYE_TEXT, COL_GCS_VERBAL_TEXT, COL_GCS_MOTOR_TEXT])
    s_eye_score, s_verbal_score, s_motor_score = map(sanitize_name, [COL_GCS_EYE_SCORE, COL_GCS_VERBAL_SCORE, COL_GCS_MOTOR_SCORE])
    s_gcs_sum = sanitize_name(COL_GCS_SUM)
    gcs_text_to_score_map = {
        s_eye_txt: {r'.*4.*|.*spontaneously.*|.*not applicable.*': 4, r'.*3.*|.*speech.*': 3, r'.*2.*|.*pain.*': 2, r'.*1.*|.*no response.*': 1},
        s_verbal_txt: {r'.*5.*|.*oriented.*|.*not applicable.*': 5, r'.*4.*|.*confused.*': 4, r'.*3.*|.*inappropriate.*': 3, r'.*2.*|.*incomprehensible.*': 2, r'.*1.*|.*no verbal.*': 1},
        s_motor_txt: {r'.*6.*|.*obeys.*': 6, r'.*5.*|.*localises.*': 5, r'.*4.*|.*withdrawal.*': 4, r'.*3.*|.*flexion.*': 3, r'.*2.*|.*extension.*': 2, r'.*1.*|.*no motor.*': 1}
    }
    if s_eye_txt in df_master.columns: df_master[s_eye_score] = pd.to_numeric(df_master[s_eye_txt].astype(str).str.lower().replace(gcs_text_to_score_map[s_eye_txt], regex=True), errors='coerce')
    if s_verbal_txt in df_master.columns: df_master[s_verbal_score] = pd.to_numeric(df_master[s_verbal_txt].astype(str).str.lower().replace(gcs_text_to_score_map[s_verbal_txt], regex=True), errors='coerce')
    if s_motor_txt in df_master.columns: df_master[s_motor_score] = pd.to_numeric(df_master[s_motor_txt].astype(str).str.lower().replace(gcs_text_to_score_map[s_motor_txt], regex=True), errors='coerce')
    if s_verbal_txt in df_master.columns:
        intub_mask = df_master[s_verbal_txt].astype(str).str.lower().str.contains('intub', na=False)
        df_master['GCSV_intubated'] = intub_mask.astype(int)
        if s_verbal_score in df_master.columns: df_master.loc[intub_mask, s_verbal_score] = np.nan
    else:
        df_master['GCSV_intubated'] = 0
    df_master[s_gcs_sum] = df_master[[c for c in [s_eye_score, s_verbal_score, s_motor_score] if c in df_master.columns]].sum(axis=1, min_count=1)

    df_before_gcs = df_master.copy()
    gcs_mask = (df_master[s_gcs_sum] >= 3) & (df_master[s_gcs_sum] <= 13)
    df_master = df_master[gcs_mask]
    df_master = log_step(df_before_gcs, df_master, "Mild TBI (GCS > 13) or GCS < 3 or GCS not calculable", "Patients with moderate/severe TBI")

    # --- Outcome Processing and Filtering ---
    s_outcome = sanitize_name(COL_OUTCOME)
    cleaned_outcomes = df_master[s_outcome].astype(str).str.lower().str.strip().replace(r'[^a-z0-9\s-]', '', regex=True)
    df_master[COL_TARGET_GOSE_1_8] = cleaned_outcomes.map(GOSE_TEXT_TO_NUMERIC_MAP)
    df_master[COL_TARGET_ORDINAL_0_6] = df_master[COL_TARGET_GOSE_1_8].map(GOSE_1_8_TO_ORDINAL_0_6_MAP)
    df_master[COL_TARGET_MORTALITY] = (df_master[COL_TARGET_GOSE_1_8] == 1).astype(int)
    df_master[COL_TARGET_FAVORABLE] = (df_master[COL_TARGET_GOSE_1_8] >= 5).astype(int)

    df_before_gose = df_master.copy()
    df_master.dropna(subset=[s_id, COL_TARGET_ORDINAL_0_6], inplace=True)
    df_master = log_step(df_before_gose, df_master, "Invalid or missing GOSE outcome", "Patients with valid GOSE")

    # --- Follow-up and Uncertainty Filtering ---
    s_follow = sanitize_name(COL_GOSE_FOLLOWUP_MONTHS)
    if s_follow in df_master.columns:
        df_before_follow = df_master.copy()
        max_months = cfg["DATA_PROCESSING"]["GOSE_FOLLOWUP_MAX_MONTHS"]
        s_follow_num = pd.to_numeric(df_master[s_follow], errors='coerce')
        df_master = df_master[s_follow_num.isna() | (s_follow_num <= max_months)]
        df_master = log_step(df_before_follow, df_master, f"Follow-up > {max_months} months", f"Patients with follow-up <= {max_months} months")

    s_uncertainty = sanitize_name(COL_TRAUMA_UNCERTAINTY)
    if s_uncertainty in df_master.columns:
        df_before_uncertainty = df_master.copy()
        max_uncertainty = cfg["DATA_PROCESSING"]["MAX_TRAUMA_UNCERTAINTY_HRS"]
        uncertainty_series = pd.to_numeric(df_master[s_uncertainty], errors='coerce')
        mask_to_keep = uncertainty_series.isna() | (uncertainty_series <= max_uncertainty)
        df_master = df_master[mask_to_keep]
        df_master = log_step(df_before_uncertainty, df_master, f"Trauma time uncertainty > {max_uncertainty} hrs", "Final analysis-ready cohort")

    # --- Finalize Data (Pupils, CT, etc.) ---
    s_trauma_dt, s_death_dt = sanitize_name(COL_TRAUMA_DATETIME), sanitize_name(COL_DATE_OF_DEATH)
    df_master[s_trauma_dt] = pd.to_datetime(df_master[s_trauma_dt], errors='coerce')
    df_master[s_death_dt] = pd.to_datetime(df_master[s_death_dt], errors='coerce')
    
    s_ct_col = sanitize_name(COL_CT_FINDINGS)
    if s_ct_col in df_master.columns:
        ct_findings_map = { 'normal': 'Normal CT', 'a sdh': 'ASDH', 'kontusioner': 'Contusion', 'traumatisk-sab': 'Traumatic SAH', 'edh': 'EDH', 'dai': 'DAI', 'impressionsfraktur': 'Impression fracture', 'mixed': 'Mixed', 'annat': 'Other' }
        df_master[s_ct_col] = df_master[s_ct_col].astype(str).str.lower().str.strip().map(ct_findings_map).fillna('Other')

    s_pupil_l, s_pupil_r = map( sanitize_name, [COL_PUPIL_L_REACT, COL_PUPIL_R_REACT])
    binary_map = {r'(?i)^\s*(ja|yes|Normal|Trög|1|1\.0)\s*$': 1, r'(?i)^\s*(nej|no|Stel|nonreactive|non-reactive|0|0\.0)\s*$': 0}
    for col in [s_pupil_l, s_pupil_r]:
        if col in df_master.columns: df_master[col] = pd.to_numeric(df_master[col].replace(binary_map, regex=True), errors='coerce')
    df_master[sanitize_name(COL_PUPIL_REACT_COMBINED)] = np.select([(df_master.get(s_pupil_l) == 1) & (df_master.get(s_pupil_r) == 1), (df_master.get(s_pupil_l) == 0) & (df_master.get(s_pupil_r) == 0)], ['Both_Reactive', 'None_Reactive'], default='One_Reactive')
    
    for h in cfg["FEATURE_ENGINEERING"]["PREDICTION_LANDMARKS_HOURS"]:
        df_master[f'Died_within_{h}h'] = ((df_master[s_death_dt] - df_master[s_trauma_dt]) <= pd.to_timedelta(h, 'h')) # .fillna(False) removed intentionally

    df_master.set_index(s_id, inplace=True)
    raw_baseline_feats = sum([ [sanitize_name(f) for f in flist] for flist in USER_PROVIDED_FEATURES.values() ], [])
    if 'GCSV_intubated' not in raw_baseline_feats: raw_baseline_feats.append('GCSV_intubated')

    # --- Train/Test Split ---
    df_master_splittable = df_master.dropna(subset=[s_trauma_dt, COL_TARGET_ORDINAL_0_6])
    
    split_strategy = cfg["DATA_PROCESSING"].get("SPLIT_STRATEGY", "temporal")
    test_frac, random_state = cfg["DATA_PROCESSING"]["TEST_SET_FRACTION"], cfg["MODELING"]["RANDOM_STATE"]

    if split_strategy == "stratified":
        train_pids, test_pids = train_test_split(df_master_splittable.index, stratify=df_master_splittable[COL_TARGET_ORDINAL_0_6], test_size=test_frac, random_state=random_state)
    elif split_strategy == "temporal":
        dates = df_master_splittable[s_trauma_dt].sort_values()
        split_date = dates.quantile(1 - test_frac, interpolation='lower')
        train_pids = df_master_splittable.index[df_master_splittable[s_trauma_dt] <= split_date]
        test_pids  = df_master_splittable.index[df_master_splittable[s_trauma_dt] > split_date]
    else:
        raise ValueError(f"Unknown SPLIT_STRATEGY: '{split_strategy}'.")

    # Return the full dataframes; we will filter them inside the main loop
    return (df_master.loc[train_pids], df_master.loc[test_pids]), raw_baseline_feats, flow_log

# ============================== FEATURE ENGINEERING =================================
# This function prepares the baseline patient data for analysis at a specific point in time
# after injury, which we call a 'landmark' (e.g., 24 hours). Its main job is to handle
# information about Decompressive Craniectomy (DC) correctly, so we don't accidentally use
# future information to make a prediction. For example, to predict an outcome at 24 hours,
# we should only know if a DC happened *before* 24 hours, not after.
def build_baseline_for_landmark(master_df: pd.DataFrame, raw_baseline_feats: List[str], h: int) -> pd.DataFrame:
    df = master_df.copy()
    s_dc_time, s_dc_type = sanitize_name(COL_DC_TIME), sanitize_name(COL_DC_TYPE)

    # start from the raw baseline list, but exclude original DC cols
    feats = [f for f in raw_baseline_feats if not f.startswith('Died_within_') and f not in {s_dc_time, s_dc_type}]

    if s_dc_time in df.columns:
        # days -> hours, drop negatives as invalid
        dc_hours = pd.to_numeric(df[s_dc_time], errors='coerce') * 24.0
        dc_hours = dc_hours.mask(dc_hours < 0)  # treat negative values as missing

        dc_by_landmark = (dc_hours <= float(h))
        df[f'DC_by_{h}h'] = dc_by_landmark.astype('float')  # 1.0/0.0

        # only reveal time if DC occurred by the landmark (censored otherwise)
        df[f'DC_time_if_by_{h}h'] = np.where(dc_by_landmark, dc_hours, np.nan)

        feats += [f'DC_by_{h}h', f'DC_time_if_by_{h}h']  # (append once)

        if s_dc_type in df.columns:
            # keep original type when DC is by the landmark, otherwise explicit "No_DC_by_landmark"
            df[f'DC_type_by_{h}h'] = np.where(dc_by_landmark, df[s_dc_type].fillna('Unknown_DC_type').astype(str), 'No_DC_by_landmark')
            feats.append(f'DC_type_by_{h}h')

    # return only the features that exist in the frame
    return df[[f for f in feats if f in df.columns]]

# ======================== ENHANCED FEATURE ENGINEERING HELPERS ========================
# This is a small helper function that standardizes time-series data. Imagine vital signs are
# recorded at irregular times (e.g., 08:03, 08:17, 08:21). This function creates a perfectly
# regular, minute-by-minute timeline for each patient and places the measurements on it. It
# also handles small gaps by carrying the last known value forward for a few minutes.
def _grid_align(s: pd.Series, grid_index: pd.TimedeltaIndex, ffill_minutes: int) -> pd.Series:
    """Align irregular Timedelta series to a 1-min grid with limited forward fill."""
    s = pd.to_numeric(s, errors='coerce').dropna()
    if s.empty:
        return pd.Series(index=grid_index, dtype=float)
    s = s.sort_index()

    # Upsample to 1-min, forward-fill short gaps (limit in minutes)
    g = s.resample('1min').ffill(limit=ffill_minutes)

    # Match the requested [0..H] grid exactly
    g = g.reindex(grid_index)
    return g


# This function calculates the 'burden' of a single physiological variable. For example,
# it can answer the question: "What percentage of the first 24 hours did this patient spend
# with an Intracranial Pressure (ICP) above 20 mmHg?". It takes the patient's data, a
# clinical threshold (e.g., 20 mmHg), and a direction ('above' or 'below') to calculate this burden,
# correctly ignoring any periods where monitoring data was missing.
def _single_burden(
    s: pd.Series,
    direction: str,
    thr: float,
    grid_index: pd.TimedeltaIndex,
    *,
    ffill_for_mask_minutes: int = 3,
    min_obs_minutes: int = 0,
    return_minutes: bool = False,
) -> float:
    """Compute burden over the FULL [0..H] window.


    Key changes vs. previous version:
    - Denominator = **truly observed minutes** (no forward-fill).
    - Condition mask continuity can use a **short ffill (default 3 min)**.
    - Optionally return **absolute minutes** instead of fraction.
    """
    if direction not in {"above", "below"}:
        return np.nan
    g_obs = _grid_align_noffill(s, grid_index)
    obs_mask = g_obs.notna()
    obs_minutes = int(obs_mask.sum())
    if obs_minutes < int(min_obs_minutes):
        return np.nan
    g_mask = _grid_align_generic(s, grid_index, ffill_minutes=ffill_for_mask_minutes)
    g_num = pd.to_numeric(g_mask, errors='coerce')
    cond = (g_num > float(thr)) if direction == "above" else (g_num < float(thr))
    hits = (cond & obs_mask).sum()
    return float(hits) if return_minutes else (float(hits) / float(obs_minutes) if obs_minutes else np.nan)

# This function calculates a 'joint burden' for two variables simultaneously. It's used to
# identify periods of secondary injury, for instance, by answering: "What percentage of
# time did the patient suffer from BOTH high ICP (above 20) AND low blood pressure (below 90)
# at the same time?". This helps us quantify the duration of more complex and dangerous physiological states.
def _joint_burden(
    s1: pd.Series,
    direction1: str,
    thr1: float,
    s2: pd.Series,
    direction2: str,
    thr2: float,
    grid_index: pd.TimedeltaIndex,
    *,
    ffill_for_mask_minutes: int = 3,
    min_obs_minutes: int = 0,
    return_minutes: bool = False,
) -> float:
    """Joint burden where BOTH variables meet their condition, over the FULL [0..H] window.


    - Denominator = minutes where **both** variables have a true observation (no ffill).
    - Masks use short ffill for continuity.
    """
    if direction1 not in {"above", "below"} or direction2 not in {"above", "below"}:
        return np.nan
    g1_obs = _grid_align_noffill(s1, grid_index)
    g2_obs = _grid_align_noffill(s2, grid_index)
    obs_mask = g1_obs.notna() & g2_obs.notna()
    obs_minutes = int(obs_mask.sum())
    if obs_minutes < int(min_obs_minutes):
        return np.nan
    g1 = _grid_align_generic(s1, grid_index, ffill_minutes=ffill_for_mask_minutes)
    g2 = _grid_align_generic(s2, grid_index, ffill_minutes=ffill_for_mask_minutes)
    m1 = (g1 > float(thr1)) if direction1 == "above" else (g1 < float(thr1))
    m2 = (g2 > float(thr2)) if direction2 == "above" else (g2 < float(thr2))
    joint = (m1 & m2 & obs_mask).sum()
    return float(joint) if return_minutes else (float(joint) / float(obs_minutes) if obs_minutes else np.nan)

# This function categorizes and counts physiological 'episodes' (like spikes in ICP) based on their
# severity (peak value) and their duration. Think of it as creating a grid where one axis is the
# peak ICP (e.g., 20-25, 25-30 mmHg) and the other is the duration (e.g., 5-15 min, 15-30 min).
# It then counts how many of a patient's episodes fall into each box, giving a summary like
# '3 short, mild episodes' and '1 long, severe episode'.
def _episode_grid_counts(peaks: List[float],
                         durs: List[int],
                         peak_bins: List[float],
                         dur_bins: List[int]) -> Dict[str, int]:
    """
    Count episodes across peak x duration bins.
    Returns dict with keys like 'count_p25_30_d5_15', using closed-open bins.
    """
    out: Dict[str, int] = {}
    if not peaks or not durs:
        return out
    p = np.asarray(peaks, dtype=float)
    d = np.asarray(durs, dtype=int)

    p_idx = np.digitize(p, peak_bins, right=False)  # 0..len(bins)
    d_idx = np.digitize(d, dur_bins, right=False)
    # Build bin label helper
    def _label_edges(edges, i, suffix):
        if i == 0:
            return f"{suffix}lt{edges[0]}"
        elif i == len(edges):
            return f"{suffix}ge{edges[-1]}"
        else:
            return f"{suffix}{edges[i-1]}_{edges[i]}"

    for i in range(p_idx.size):
        pi, di = p_idx[i], d_idx[i]
        p_lab = _label_edges(peak_bins, pi, "p")
        d_lab = _label_edges(dur_bins,  di, "d")
        key = f"count_{p_lab}_{d_lab}"
        out[key] = out.get(key, 0) + 1
    return out

# -------------------------- EVENT SIGNATURE HELPERS ---------------------------

# This is a helper function that identifies 'episodes' from a minute-by-minute data stream.
# For example, if we have a record of when a patient's ICP was above 20 mmHg, this function
# will find the start and end times of each continuous period of high ICP. It can also merge
# two episodes that are very close together and ignore very short spikes that might just be noise.
def _extract_episode_runs(mask: pd.Series, min_len: int = 1, merge_gap: int = 0) -> List[Tuple[int, int]]:
    """
    Given a boolean mask over a 1-min grid (index 0..H in TimedeltaIndex), return
    list of (start_idx, end_idx) inclusive minute indices of True-runs.
    min_len: minimum run length (in minutes) to keep.
    merge_gap: merge two runs if separated by <= merge_gap minutes of False.
    """
    if mask is None or mask.empty:
        return []
    m = mask.fillna(False).to_numpy(dtype=bool)
    n = m.size
    runs = []
    i = 0
    while i < n:
        if not m[i]:
            i += 1
            continue
        j = i
        while j + 1 < n and m[j + 1]:
            j += 1
        runs.append([i, j])  # inclusive
        i = j + 1

    # Merge runs separated by small gaps
    if merge_gap > 0 and runs:
        merged = [runs[0]]
        for s, e in runs[1:]:
            prev_s, prev_e = merged[-1]
            gap = s - prev_e - 1
            if gap <= merge_gap:
                merged[-1][1] = e
            else:
                merged.append([s, e])
        runs = merged

    # Enforce minimum duration
    kept = []
    for s, e in runs:
        dur = (e - s + 1)
        if dur >= int(min_len):
            kept.append((s, e))
    return kept

# Once the episodes (like high ICP events) have been identified, this powerful function calculates
# a detailed summary of them. It's like creating a clinical signature of the patient's instability.
# It measures things like: the total number of episodes, how long they lasted on average, the most
# severe value reached (e.g., peak ICP), how long it took for the first episode to occur, and even
# how long it took for the patient's physiology to return to a 'safe' level after an episode.
def _episode_metrics_from_mask(values: pd.Series,
                               mask: pd.Series,
                               grid_index: pd.TimedeltaIndex,
                               *,
                               peak_mode: str = "max",
                               recovery_to: Optional[float] = None,
                               recovery_stability: int = 5) -> Dict[str, Any]:
    """
    Compute episode metrics for a 1-min grid-aligned series.
    values: numeric series aligned to 1-min grid [0..H].
    mask: boolean condition (e.g., ICP > 25) on the same grid.
    peak_mode: "max" (for 'above' insults like ICP) or "min" (for 'below' like CPP).
    recovery_to: if provided, measure time from episode peak to first time series
                 stays beyond 'recovery_to' in the *non-insult* direction for
                 'recovery_stability' consecutive minutes.
                 e.g., after ICP>25 spike, recovery_to=20 means “time to be <20
                 for ≥5 min”.
    Returns a dict with counts, durations, peaks, time-to-first, clustering, recovery.
    """
    out: Dict[str, Any] = {}
    if values is None or values.empty or mask is None or mask.empty:
        # Standardized empties
        out.update({
            "n_episodes": 0,
            "tot_mins": 0.0,
            "mean_dur_mins": np.nan,
            "median_dur_mins": np.nan,
            "p90_dur_mins": np.nan,
            "longest_mins": np.nan,
            "time_to_first_min": np.nan,
            "peak_max": np.nan,
            "peak_mean": np.nan,
            "iei_mean_mins": np.nan,
            "iei_cv": np.nan,
            "iei_burstiness": np.nan,
            "recovery_median_mins": np.nan,
            "recovery_mean_mins": np.nan,
        })
        return out

    g = pd.to_numeric(values, errors='coerce')
    cond = mask.fillna(False) & g.notna()

    # Build runs (episodes)
    runs = _extract_episode_runs(cond, min_len=1, merge_gap=0)
    n = len(runs)
    out["n_episodes"] = int(n)
    if n == 0:
        out.update({
            "tot_mins": 0.0,
            "mean_dur_mins": 0.0,
            "median_dur_mins": 0.0,
            "p90_dur_mins": 0.0,
            "longest_mins": 0.0,
            "time_to_first_min": np.nan,
            "peak_max": np.nan,
            "peak_mean": np.nan,
            "iei_mean_mins": np.nan,
            "iei_cv": np.nan,
            "iei_burstiness": np.nan,
            "recovery_median_mins": np.nan,
            "recovery_mean_mins": np.nan,
        })
        return out

    # Durations and time-to-first
    durs = [(e - s + 1) for s, e in runs]
    out["tot_mins"] = float(np.sum(durs))
    out["mean_dur_mins"] = float(np.mean(durs))
    out["median_dur_mins"] = float(np.median(durs))
    out["p90_dur_mins"] = float(np.quantile(durs, 0.90))
    out["longest_mins"] = float(np.max(durs))
    # time-to-first (episode start minute from 0)
    first_start_idx = runs[0][0]
    # grid index is TimedeltaIndex at 1min resolution
    out["time_to_first_min"] = float(first_start_idx)

    # Peaks within episodes
    peaks = []
    for s, e in runs:
        seg = g.iloc[s:e+1]
        if peak_mode == "min":
            peaks.append(float(seg.min()))
        else:
            peaks.append(float(seg.max()))
    out["peak_max"] = float(np.max(peaks))
    out["peak_mean"] = float(np.mean(peaks))

    # Inter-episode intervals (minutes between end of i and start of i+1)
    if n > 1:
        iei = [runs[i+1][0] - runs[i][1] - 1 for i in range(n - 1)]
        iei_arr = np.asarray(iei, dtype=float)
        out["iei_mean_mins"] = float(iei_arr.mean())
        out["iei_cv"] = float(iei_arr.std(ddof=1) / iei_arr.mean()) if iei_arr.mean() > 0 else np.nan
        out["iei_burstiness"] = _burstiness_from_intervals(iei)
    else:
        out["iei_mean_mins"] = np.nan
        out["iei_cv"] = np.nan
        out["iei_burstiness"] = np.nan

    # Recovery metrics (optional)
    if recovery_to is None:
        out["recovery_median_mins"] = np.nan
        out["recovery_mean_mins"] = np.nan
        return out

    rec_times = []
    for s, e in runs:
        seg = g.iloc[s:e+1]
        # find peak idx within the episode
        peak_idx_local = int(np.argmin(seg.values)) if peak_mode == "min" else int(np.argmax(seg.values))
        peak_idx = s + peak_idx_local

        # from peak onward, find first time we are in the "recovered" zone for stability consecutive minutes
        if peak_mode == "min":
            # for 'min' insults (e.g., CPP low), recovery means values >= recovery_to
            rec_mask = g >= float(recovery_to)
        else:
            # for 'max' insults (e.g., ICP high), recovery means values <= recovery_to
            rec_mask = g <= float(recovery_to)

        # need stability minutes in a row
        rec_seq = rec_mask.iloc[peak_idx:]
        if rec_seq.size >= recovery_stability:
            # rolling window of stability
            stable = rec_seq.rolling(window=recovery_stability, min_periods=recovery_stability).apply(lambda x: 1.0 if np.all(x) else 0.0, raw=True)
            first_ok = stable[stable == 1.0].index.min()
            if pd.notna(first_ok):
                # time difference in minutes
                peak_time = grid_index[peak_idx]
                rec_time = first_ok
                rec_minutes = int((rec_time - peak_time) / pd.Timedelta(minutes=1))
                if rec_minutes >= 0:
                    rec_times.append(rec_minutes)

    if len(rec_times) > 0:
        out["recovery_median_mins"] = float(np.median(rec_times))
        out["recovery_mean_mins"] = float(np.mean(rec_times))
    else:
        out["recovery_median_mins"] = np.nan
        out["recovery_mean_mins"] = np.nan

    return out

# This is a key function that takes all the raw, minute-by-minute ICU monitoring data for a single
# patient and transforms it into a comprehensive set of summary features. It's the engine that
# creates the patient's 'dynamic physiological profile' for the model to use. It calculates everything
# from simple averages (like mean ICP over 24 hours) to highly sophisticated metrics like the 'burden'
# of high ICP, the 'dose' of low blood pressure, and the detailed 'episode signatures' described above.
def create_dynamic_features_from_patient_data(
    patient_icu_df: pd.DataFrame,
    landmark_h: int,
    cfg: Dict
) -> pd.Series:
    fe_cfg = cfg["FEATURE_ENGINEERING"]
    all_possible_dyn_vars = [sanitize_name(v) for v in fe_cfg["DYNAMIC_VARS"]]
    all_features: Dict[str, Any] = {}


    # meta flags: present at least one value anywhere in [0..H]
    for var in all_possible_dyn_vars:
        has_data = (
            (patient_icu_df is not None)
            and (var in patient_icu_df.columns)
            and (patient_icu_df[var].notna().any())
        )
        all_features[f"meta_has_{var}"] = int(has_data)


    if patient_icu_df is None or patient_icu_df.empty:
        return pd.Series(all_features, dtype=float)


    ffill_for_means = int(fe_cfg.get("MAX_FFILL_MINUTES", 15))
    ffill_for_masks = int(
        fe_cfg.get("FFILL_FOR_EPISODES_MINUTES", 3)
    ) if fe_cfg.get("FFILL_FOR_EPISODES_MINUTES") is not None else 3
    min_pts = int(fe_cfg.get("MIN_DATA_POINTS_PER_VAR", 5))

    # Minimum observed minutes for burdens/episodes (stricter at 48h)
    min_obs_minutes = 180 if int(landmark_h) <= 24 else 240 # 3h @24h; 4h @48h


    H = int(landmark_h)
    grid_index = _minute_grid(H) # includes the H:00 minute


    dyn_vars_present = [v for v in all_possible_dyn_vars if all_features.get(f"meta_has_{v}", 0) == 1]
    if not dyn_vars_present:
        return pd.Series(all_features, dtype=float)


    total_minutes = len(grid_index)

    # ---------- Summary + windowed features + sampling intensity ----------
    for var in dyn_vars_present:
        series_raw = pd.to_numeric(patient_icu_df[var], errors="coerce").dropna()
        if series_raw.size < min_pts:
            continue


        # No-ffill grid (true observations) & smoothed grid for means
        g_obs = _grid_align_noffill(series_raw, grid_index)
        g_mean = _grid_align_generic(series_raw, grid_index, ffill_minutes=ffill_for_means)


        obs_mask = g_obs.notna()
        obs_minutes = int(obs_mask.sum())
        coverage_frac = float(obs_minutes) / float(total_minutes) if total_minutes else 0.0


        # Sampling intensity features
        all_features[f"dyn_{var}_observed_minutes"] = float(obs_minutes)
        all_features[f"dyn_{var}_coverage_frac"] = float(coverage_frac)
        # gaps (>=15 min) & max gap
        if obs_minutes > 0:
            obs_idx = np.flatnonzero(obs_mask.values)
            gaps = np.diff(obs_idx) - 1 # minutes between observed points
            gaps = gaps[gaps >= 1]
            all_features[f"dyn_{var}_max_gap_mins"] = float(gaps.max()) if gaps.size else 0.0
            all_features[f"dyn_{var}_n_gaps_ge_15m"] = int((gaps >= 15).sum())
        else:
            all_features[f"dyn_{var}_max_gap_mins"] = np.nan
            all_features[f"dyn_{var}_n_gaps_ge_15m"] = np.nan

        # Global raw summaries
        all_features[f"dyn_{var}_mean"] = float(series_raw.mean())
        all_features[f"dyn_{var}_std"] = float(series_raw.std(ddof=0)) if series_raw.size >= 2 else 0.0
        all_features[f"dyn_{var}_q90"] = float(series_raw.quantile(0.90))
        all_features[f"dyn_{var}_q10"] = float(series_raw.quantile(0.10))
        all_features[f"dyn_{var}_count"] = int(series_raw.size)


        # Time-weighted mean via ffilled grid then drop NaN
        g_mean_nonan = g_mean.dropna()
        if g_mean_nonan.size:
            all_features[f"dyn_{var}_time_w_mean"] = float(g_mean_nonan.mean())
            all_features[f"dyn_{var}_coverage_hrs"] = float(g_mean_nonan.size / 60.0)
        else:
            all_features[f"dyn_{var}_coverage_hrs"] = 0.0


        # Sub-windows (begin/mid/end) — mid only when H==48
        begin_h = max(0, int(fe_cfg.get("BEGIN_WINDOW_HOURS", min(H, 24))))
        end_h = max(0, int(fe_cfg.get("END_WINDOW_HOURS", 8)))
        end_start_h = max(0, H - end_h)


        # BEGIN: [0, begin_h]
        if begin_h > 0:
            begin_g = g_mean.loc[:pd.to_timedelta(begin_h, "h")].dropna()
            nb = int(begin_g.size)
            if nb > 0:
                all_features[f"dyn_{var}_begin_mean"] = float(begin_g.mean())
                if nb >= 2:
                    xh = begin_g.index.total_seconds() / 3600.0
                    all_features[f"dyn_{var}_begin_slope"] = float(np.polyfit(xh, begin_g.values, 1)[0])


        # MID: [24, 40] @ 48h only
        if H == 48:
            mid_g = g_mean.loc[pd.to_timedelta(24, "h"):pd.to_timedelta(40, "h")].dropna()
            nm = int(mid_g.size)
            if nm > 0:
                all_features[f"dyn_{var}_mid_mean"] = float(mid_g.mean())
                if nm >= 2:
                    xh = mid_g.index.total_seconds() / 3600.0
                    all_features[f"dyn_{var}_mid_slope"] = float(np.polyfit(xh, mid_g.values, 1)[0])


        # END: [H-end_h, H]
        if end_h > 0:
            end_g = g_mean.loc[pd.to_timedelta(end_start_h, "h"):pd.to_timedelta(H, "h")].dropna()
            ne = int(end_g.size)
            if ne > 0:
                all_features[f"dyn_{var}_end_mean"] = float(end_g.mean())
                if ne >= 2:
                    all_features[f"dyn_{var}_end_var"] = float(end_g.var(ddof=0))
                    xh = end_g.index.total_seconds() / 3600.0
                    all_features[f"dyn_{var}_end_slope"] = float(np.polyfit(xh, end_g.values, 1)[0])


        # Simple deltas (end - begin) when both exist
        bkey, ekey = f"dyn_{var}_begin_mean", f"dyn_{var}_end_mean"
        if bkey in all_features and ekey in all_features:
            try:
                all_features[f"dyn_{var}_delta_end_minus_begin"] = float(all_features[ekey] - all_features[bkey])
            except Exception:
                pass



    # ---------- Burdens & Doses over full window, plus early/late at 48h ----------
    def _min_obs_by_h(h: int) -> int:
        return 180 if int(h) <= 24 else 240


    # Map lower-case keys to actual column names present
    name_map = {v.lower(): v for v in dyn_vars_present}


    # Single-variable burdens (fraction + minutes)
    for var_key, conditions in fe_cfg["EVENT_BURDEN_VARS"].items():
        true_var = name_map.get(var_key.lower())
        if true_var is None:
            continue
        s_var = pd.to_numeric(patient_icu_df[true_var], errors="coerce")
        for direction, thresholds in conditions.items():
            for t in thresholds:
                # FULL window
                val_frac = _single_burden(
                    s_var,
                    direction,
                    float(t),
                    grid_index,
                    ffill_for_mask_minutes=ffill_for_masks,
                    min_obs_minutes=_min_obs_by_h(H),
                    return_minutes=False,
                )
                if pd.notna(val_frac):
                    all_features[f"evt_{true_var}_frac_{direction}_{t}"] = float(val_frac)
                val_abs = _single_burden(
                    s_var,
                    direction,
                    float(t),
                    grid_index,
                    ffill_for_mask_minutes=ffill_for_masks,
                    min_obs_minutes=_min_obs_by_h(H),
                    return_minutes=True,
                )
                if pd.notna(val_abs):
                    all_features[f"evt_{true_var}_abs_mins_{direction}_{t}"] = float(val_abs)



                # EARLY/LATE at 48h (non-overlapping: early=0..23:59, late=24:00..48:00)
                if H == 48:
                    early_grid = pd.timedelta_range(start="0s", end="23:59:00", freq="1min")
                    late_grid = pd.timedelta_range(start="24:00:00", end="48:00:00", freq="1min")
                    val_frac_e = _single_burden(
                        s_var,
                        direction,
                        float(t),
                        early_grid,
                        ffill_for_mask_minutes=ffill_for_masks,
                        min_obs_minutes=120,
                        return_minutes=False,
                    )
                    if pd.notna(val_frac_e):
                        all_features[f"evt_{true_var}_frac_{direction}_{t}_early"] = float(val_frac_e)
                    val_frac_l = _single_burden(
                        s_var,
                        direction,
                        float(t),
                        late_grid,
                        ffill_for_mask_minutes=ffill_for_masks,
                        min_obs_minutes=120,
                        return_minutes=False,
                    )
                    if pd.notna(val_frac_l):
                        all_features[f"evt_{true_var}_frac_{direction}_{t}_late"] = float(val_frac_l)

    # Doses (AUC-like)
    if "icpm" in name_map:
        g_icp_obs = _grid_align_noffill(patient_icu_df[name_map["icpm"]], grid_index)
        g_icp = _grid_align_generic(patient_icu_df[name_map["icpm"]], grid_index, ffill_minutes=ffill_for_means)
        if g_icp.notna().any():
            obs_minutes_icp = int(g_icp_obs.notna().sum())
            # subtract the inclusive endpoint when converting minutes -> hours
            obs_hours = max(1.0, max(0, obs_minutes_icp - 1) / 60.0)


            for thr in fe_cfg.get("ICP_AUC_THRESHOLDS", [10, 20]):
                thr = float(thr)
                exc = (g_icp - thr).clip(lower=0)
                base = f"dose_ICPm_auc_gt_{int(thr)}"
                all_features[base] = float(exc.sum())
                all_features[base + "_per_hr"] = float(exc.sum()) / obs_hours


                if H == 48:
                    early_slice = g_icp.loc[:pd.to_timedelta("23:59:00")]
                    late_slice = g_icp.loc[pd.to_timedelta("24:00:00"):]
                    if early_slice.notna().any():
                        e_exc = (early_slice - thr).clip(lower=0)
                        all_features[base + "_early"] = float(e_exc.sum())
                    if late_slice.notna().any():
                        l_exc = (late_slice - thr).clip(lower=0)
                        all_features[base + "_late"] = float(l_exc.sum())


    if "cpp" in name_map:
        g_cpp_obs = _grid_align_noffill(patient_icu_df[name_map["cpp"]], grid_index)
        g_cpp = _grid_align_generic(patient_icu_df[name_map["cpp"]], grid_index, ffill_minutes=ffill_for_means)
        if g_cpp.notna().any():
            cpp60 = (60 - g_cpp).clip(lower=0)
            cpp50 = (50 - g_cpp).clip(lower=0)
            obs_minutes_cpp = int(g_cpp_obs.notna().sum())
            obs_hours_cpp = max(1.0, max(0, obs_minutes_cpp - 1) / 60.0)
            all_features["dose_CPP_auc_lt_60"] = float(cpp60.sum())
            all_features["dose_CPP_auc_lt_50"] = float(cpp50.sum())
            all_features["dose_CPP_auc_lt_60_per_hr"] = float(cpp60.sum()) / obs_hours_cpp
            all_features["dose_CPP_auc_lt_50_per_hr"] = float(cpp50.sum()) / obs_hours_cpp
            if H == 48:
                early_slice = g_cpp.loc[:pd.to_timedelta("23:59:00")]
                late_slice = g_cpp.loc[pd.to_timedelta("24:00:00"):]
                if early_slice.notna().any():
                    e60 = (60 - early_slice).clip(lower=0)
                    e50 = (50 - early_slice).clip(lower=0)
                    all_features["dose_CPP_auc_lt_60_early"] = float(e60.sum())
                    all_features["dose_CPP_auc_lt_50_early"] = float(e50.sum())
                if late_slice.notna().any():
                    l60 = (60 - late_slice).clip(lower=0)
                    l50 = (50 - late_slice).clip(lower=0)
                    all_features["dose_CPP_auc_lt_60_late"] = float(l60.sum())
                    all_features["dose_CPP_auc_lt_50_late"] = float(l50.sum())



    # ---------- Event signature features (episodes / recovery) ----------
    es_cfg = fe_cfg.get("EVENT_SIGNATURE", {})
    icp_thr_main = float(es_cfg.get("ICP_GT_MAIN", 20.0))
    icp_recovery_to = float(es_cfg.get("RECOVERY_TO_ICP", 20.0))
    cpp_low_main = float(es_cfg.get("CPP_LT_MAIN", 50.0))
    prx_thr_main = float(es_cfg.get("PRX_GT_MAIN", 0.3))
    min_episode_min = int(es_cfg.get("MIN_EPISODE_MINUTES", 5))
    merge_gap_min = int(es_cfg.get("MERGE_GAP_MINUTES", 2)) # modest merging
    recovery_stability = int(es_cfg.get("RECOVERY_STABILITY_MIN", 5))


    g_icp = _grid_align_generic(patient_icu_df[name_map["icpm"]], grid_index, ffill_minutes=ffill_for_masks) if "icpm" in name_map else None
    g_cpp = _grid_align_generic(patient_icu_df[name_map["cpp"]], grid_index, ffill_minutes=ffill_for_masks) if "cpp" in name_map else None
    g_prx = _grid_align_generic(patient_icu_df[name_map["prx"]], grid_index, ffill_minutes=ffill_for_masks) if "prx" in name_map else None


    # Gate PRx-specific metrics behind min observation time
    def _enough_obs(var_key: str, minutes_needed: int = min_obs_minutes) -> bool:
        col = name_map.get(var_key)
        if not col:
            return False
        g_noff = _grid_align_noffill(patient_icu_df[col], grid_index)
        return int(g_noff.notna().sum()) >= minutes_needed


    # ICP > 20 episodes
    if g_icp is not None and g_icp.notna().any():
        icp_mask = g_icp > icp_thr_main
        base_runs = _extract_episode_runs(icp_mask, min_len=max(1, min_episode_min), merge_gap=merge_gap_min)
        if base_runs:
            keep = pd.Series(False, index=icp_mask.index)
            for s_idx, e_idx in base_runs:
                keep.iloc[s_idx:e_idx + 1] = True
            icp_mask = keep
        icp_epi = _episode_metrics_from_mask(
            g_icp,
            icp_mask,
            grid_index,
            peak_mode="max",
            recovery_to=icp_recovery_to,
            recovery_stability=recovery_stability,
        )
        for k, v in icp_epi.items():
            all_features[f"evt_ICPm_gt{int(icp_thr_main)}_{k}"] = float(v) if pd.notna(v) else (0.0 if k in {"n_episodes", "tot_mins"} else np.nan)

        runs = _extract_episode_runs(icp_mask, min_len=1, merge_gap=0)
        if runs:
            durs = [(e - s + 1) for s, e in runs]
            peaks = [float(g_icp.iloc[s:e + 1].max()) for s, e in runs]
            peak_bins = es_cfg.get("ICP_PEAK_BINS", [25.0, 30.0, 40.0])
            dur_bins = es_cfg.get("DURATION_BINS_MIN", [5, 15, 60])
            grid_counts = _episode_grid_counts(peaks, durs, list(map(float, peak_bins)), list(map(int, dur_bins)))
            for k, v in grid_counts.items():
                all_features[f"evtgrid_ICPm_gt{int(icp_thr_main)}_{k}"] = int(v)

# =========================================================================
# Double & Triple Hit Episodes (Looping over multiple ICP thresholds)
# =========================================================================
    joint_icp_thresholds = es_cfg.get("JOINT_EVENT_ICP_THRESHOLDS", [icp_thr_main])
    prx_tag = str(prx_thr_main).replace('.', '')  # e.g., 0.3 -> "03"

    has_icp_data = (g_icp is not None) and g_icp.notna().any()
    has_cpp_data = (g_cpp is not None) and g_cpp.notna().any()
    has_prx_data = (g_prx is not None) and g_prx.notna().any() and _enough_obs("prx")

    # --- Double-hit 1: ICP–CPP (ICP > T & CPP < low) ---
    if has_icp_data and has_cpp_data:
        for icp_thr in joint_icp_thresholds:
            dh_icp_cpp_mask = (
                (g_icp > icp_thr) &
                (g_cpp < cpp_low_main) &
                g_icp.notna() & g_cpp.notna()
            )
            dh_icp_cpp_runs = _extract_episode_runs(
                dh_icp_cpp_mask,
                min_len=max(1, min_episode_min),
                merge_gap=merge_gap_min
            )
            if dh_icp_cpp_runs:
                keep = pd.Series(False, index=dh_icp_cpp_mask.index)
                for s_idx, e_idx in dh_icp_cpp_runs:
                    keep.iloc[s_idx:e_idx + 1] = True
                dh_icp_cpp_mask = keep

            dh_icp_cpp_epi = _episode_metrics_from_mask(
                g_icp, dh_icp_cpp_mask, grid_index,
                peak_mode="max",
                recovery_to=icp_recovery_to,
                recovery_stability=recovery_stability
            )
            for k, v in dh_icp_cpp_epi.items():
                fname = f"evt_doublehit_icp{int(icp_thr)}_cpp{int(cpp_low_main)}_{k}"
                all_features[fname] = float(v) if pd.notna(v) else (0.0 if k in {"n_episodes","tot_mins"} else np.nan)

    # --- Double-hit 2: ICP–PRx (ICP > T & PRx > thr) ---
    if has_icp_data and has_prx_data:
        for icp_thr in joint_icp_thresholds:
            dh_icp_prx_mask = (
                (g_icp > icp_thr) &
                (g_prx > prx_thr_main) &
                g_icp.notna() & g_prx.notna()
            )
            dh_icp_prx_runs = _extract_episode_runs(
                dh_icp_prx_mask,
                min_len=max(1, min_episode_min),
                merge_gap=merge_gap_min
            )
            if dh_icp_prx_runs:
                keep = pd.Series(False, index=dh_icp_prx_mask.index)
                for s_idx, e_idx in dh_icp_prx_runs:
                    keep.iloc[s_idx:e_idx + 1] = True
                dh_icp_prx_mask = keep

            dh_icp_prx_epi = _episode_metrics_from_mask(
                g_icp, dh_icp_prx_mask, grid_index,
                peak_mode="max",
                recovery_to=icp_recovery_to,
                recovery_stability=recovery_stability
            )
            for k, v in dh_icp_prx_epi.items():
                fname = f"evt_doublehit_icp{int(icp_thr)}_prx{prx_tag}_{k}"
                all_features[fname] = float(v) if pd.notna(v) else (0.0 if k in {"n_episodes","tot_mins"} else np.nan)

    # --- Triple-hit: ICP–CPP–PRx (ICP > T & CPP < low & PRx > thr) ---
    if has_icp_data and has_cpp_data and has_prx_data:
        for icp_thr in joint_icp_thresholds:
            th_mask = (
                (g_icp > icp_thr) &
                (g_cpp < cpp_low_main) &
                (g_prx > prx_thr_main) &
                g_icp.notna() & g_cpp.notna() & g_prx.notna()
            )
            th_runs = _extract_episode_runs(
                th_mask,
                min_len=max(1, min_episode_min),
                merge_gap=merge_gap_min
            )
            if th_runs:
                keep = pd.Series(False, index=th_mask.index)
                for s_idx, e_idx in th_runs:
                    keep.iloc[s_idx:e_idx + 1] = True
                th_mask = keep

            th_epi = _episode_metrics_from_mask(
                g_icp, th_mask, grid_index,
                peak_mode="max",
                recovery_to=icp_recovery_to,
                recovery_stability=recovery_stability
            )
            for k, v in th_epi.items():
                fname = f"evt_triplehit_icp{int(icp_thr)}_cpp{int(cpp_low_main)}_prx{prx_tag}_{k}"
                all_features[fname] = float(v) if pd.notna(v) else (0.0 if k in {"n_episodes","tot_mins"} else np.nan)

    # Optional joint burdens using PRx (only if enough PRx observed)
    prx_col = name_map.get("prx")
    if prx_col and _enough_obs("prx"):
        combos = [
            (("ICPm", "above", 20), (prx_col, "above", 0.3), "red_icp20_prx03"),
            (("CPP", "below", 50), (prx_col, "above", 0.3), "red_cpp50_prx03"),
        ]
        for (v1_name, dir1, t1), (v2_name, dir2, t2), out_name in combos:
            v1_col = name_map.get(v1_name.lower())
            v2_col = prx_col
            if v1_col and (v1_col in patient_icu_df.columns) and (v2_col in patient_icu_df.columns):
                val_frac = _joint_burden(
                    patient_icu_df[v1_col],
                    dir1,
                    t1,
                    patient_icu_df[v2_col],
                    dir2,
                    t2,
                    grid_index,
                    ffill_for_mask_minutes=ffill_for_masks,
                    min_obs_minutes=_min_obs_by_h(H),
                    return_minutes=False,
                )
                if pd.notna(val_frac):
                    all_features[f"evt_{out_name}_frac"] = float(val_frac)
                val_abs = _joint_burden(
                    patient_icu_df[v1_col],
                    dir1,
                    t1,
                    patient_icu_df[v2_col],
                    dir2,
                    t2,
                    grid_index,
                    ffill_for_mask_minutes=ffill_for_masks,
                    min_obs_minutes=_min_obs_by_h(H),
                    return_minutes=True,
                )
                if pd.notna(val_abs):
                    all_features[f"evt_{out_name}_abs_mins"] = float(val_abs)



    return pd.Series(all_features)

# This is a statistical helper that measures the 'burstiness' of events. Given the time gaps
# between episodes (e.g., high ICP spikes), it calculates a score that tells us if the episodes
# are occurring randomly and periodically, or if they tend to happen in tight clusters or 'bursts'.
def _burstiness_from_intervals(intervals: List[int]) -> float:
    """
    Burstiness B = (sigma - mu) / (sigma + mu); returns np.nan if not defined.
    """
    if intervals is None or len(intervals) < 2:
        return np.nan
    arr = np.asarray(intervals, dtype=float)
    mu = arr.mean()
    sd = arr.std(ddof=1) if arr.size > 1 else 0.0
    if (sd + mu) == 0:
        return np.nan
    return float((sd - mu) / (sd + mu))

# -------------- ICU feature signal strength audit:   --------------------
def _write_df_safe(df: pd.DataFrame, path: str):
    try:
        df.to_csv(path, index=False)
        logging.info(f"[AUDIT] Wrote → {path}")
    except Exception as e:
        logging.warning(f"[AUDIT] Could not write {path}: {e}")

def _summ_stats(x: pd.Series, any_mask: Optional[pd.Series] = None) -> Dict[str, Any]:
    x = pd.to_numeric(x, errors='coerce')
    out = {"n": int(x.notna().sum()),
           "mean": float(x.mean()) if x.notna().any() else np.nan,
           "median": float(x.median()) if x.notna().any() else np.nan,
           "p25": float(x.quantile(0.25)) if x.notna().any() else np.nan,
           "p75": float(x.quantile(0.75)) if x.notna().any() else np.nan}
    if any_mask is not None and any_mask.notna().any():
        xa = x[any_mask.fillna(False)]
        out.update({
            "median_if_any": float(xa.median()) if xa.notna().any() else np.nan,
            "p25_if_any": float(xa.quantile(0.25)) if xa.notna().any() else np.nan,
            "p75_if_any": float(xa.quantile(0.75)) if xa.notna().any() else np.nan,
        })
    return out

def audit_event_burdens_and_episodes(icu_df: pd.DataFrame, h: int, cfg: Dict, split_label: str):
    """
    Summarize 'how much signal exists' in ICU features:
      - Burdens: evt_{VAR}_{frac|abs_mins}_{above|below}_{thr}[_early|_late]
      - ICP episodes: evt_ICPm_gt{thr}_{n_episodes|tot_mins|...}
      - ICP episode grid: evtgrid_ICPm_gt{thr}_count_pXX_YY_dAA_BB
      - Joint events: evt_doublehit_..._n_episodes, evt_triplehit_..._n_episodes
    Writes CSVs and quick hists into RUN_INFO/OUTPUT_FOLDER.
    """
    if icu_df is None or icu_df.empty:
        logging.info(f"[AUDIT {h}h/{split_label}] ICU frame empty; skipping.")
        return

    out_dir = cfg["RUN_INFO"]["OUTPUT_FOLDER"]
    cols = icu_df.columns.tolist()
    n_patients = len(icu_df)

    # ---------------- Burdens ----------------
    burden_rows = []
    pat = re.compile(r"^evt_(?P<var>[A-Za-z0-9_]+)_(?P<kind>frac|abs_mins)_(?P<dir>above|below)_(?P<thr>-?\d+(?:\.\d+)?)"
                     r"(?:_(?P<phase>early|late))?$", re.I)
    # stash matching columns by key so we can pair frac & abs mins
    buckets: Dict[Tuple[str,str,str,str], Dict[str, pd.Series]] = {}
    for c in cols:
        m = pat.match(c)
        if not m: 
            continue
        var = m.group("var")
        kind = m.group("kind").lower()
        direction = m.group("dir").lower()
        thr = m.group("thr")
        phase = m.group("phase") or ""
        key = (var, direction, thr, phase)
        buckets.setdefault(key, {})[kind] = icu_df[c]

    for (var, direction, thr, phase), d in buckets.items():
        frac = d.get("frac")
        mins = d.get("abs_mins")
        any_mask = None
        if mins is not None:
            any_mask = pd.to_numeric(mins, errors='coerce') > 0
        elif frac is not None:
            any_mask = pd.to_numeric(frac, errors='coerce') > 0

        row = {
            "landmark_h": h, "split": split_label,
            "variable": var, "direction": direction, "threshold": thr, "phase": (phase or "full"),
            "n_patients": n_patients,
            "n_with_any": int(any_mask.sum()) if any_mask is not None else np.nan,
            "pct_with_any": (float(any_mask.sum()) / n_patients * 100.0) if any_mask is not None and n_patients else np.nan,
        }
        if mins is not None:
            row.update({f"mins_{k}": v for k, v in _summ_stats(mins, any_mask).items()})
        if frac is not None:
            row.update({f"frac_{k}": v for k, v in _summ_stats(frac, any_mask).items()})
        burden_rows.append(row)

    df_burden = pd.DataFrame(burden_rows).sort_values(["variable","direction","threshold","phase"])
    _write_df_safe(df_burden, os.path.join(out_dir, f"icu_event_burdens_{split_label}_{h}h.csv"))

    # ---------------- ICP Episodes (gt main thr) ----------------
    epi = {}
    # find the threshold tag, e.g., evt_ICPm_gt25_n_episodes → thr_tag="gt25"
    epi_n_cols = [c for c in cols if c.startswith("evt_ICPm_gt") and c.endswith("_n_episodes")]
    if epi_n_cols:
        # assume same thr tag across metrics
        base = epi_n_cols[0].split("_n_episodes")[0]  # evt_ICPm_gt25
        thr_tag = base.replace("evt_ICPm_", "")       # gt25
        col_n = f"evt_ICPm_{thr_tag}_n_episodes"
        col_tot = f"evt_ICPm_{thr_tag}_tot_mins"
        # Collect available metrics safely
        def _col(name): return icu_df[name] if name in icu_df.columns else pd.Series([np.nan]*n_patients, index=icu_df.index)
        s_n = _col(col_n)
        s_tot = _col(col_tot)

        any_icp = pd.to_numeric(s_n, errors='coerce') > 0
        epi["landmark_h"] = h
        epi["split"] = split_label
        epi["thr_tag"] = thr_tag
        epi["n_patients"] = n_patients
        epi["patients_with_any_icp_ep"] = int(any_icp.sum())
        epi["pct_with_any_icp_ep"] = float(any_icp.mean()*100.0) if n_patients else np.nan
        epi["total_icp_episodes"] = int(pd.to_numeric(s_n, errors='coerce').fillna(0).sum())
        # per-patient distributions
        epi.update({f"n_episodes_{k}": v for k, v in _summ_stats(s_n, any_icp).items()})
        epi.update({f"tot_mins_{k}": v for k, v in _summ_stats(s_tot, any_icp).items()})

        # optional extra summaries if present
        for extra in ["longest_mins","peak_max","peak_mean","time_to_first_min","recovery_median_mins","iei_mean_mins","iei_cv","iei_burstiness"]:
            c = f"evt_ICPm_{thr_tag}_{extra}"
            if c in icu_df.columns:
                epi.update({f"{extra}_{k}": v for k, v in _summ_stats(icu_df[c], any_icp).items()})

        df_epi = pd.DataFrame([epi])
        _write_df_safe(df_epi, os.path.join(out_dir, f"icu_icp_episode_summary_{split_label}_{h}h.csv"))

        # quick hists
        try:
            fig, ax = plt.subplots(figsize=(6,4))
            pd.to_numeric(s_n, errors='coerce').fillna(0).plot(kind='hist', bins=20, ax=ax)
            ax.set_title(f"ICP {thr_tag} – # episodes per patient ({split_label}, {h}h)")
            ax.set_xlabel("# episodes"); plt.tight_layout()
            plt.savefig(os.path.join(out_dir, f"icu_icp_n_episodes_hist_{split_label}_{h}h.png")); plt.close(fig)

            fig, ax = plt.subplots(figsize=(6,4))
            pd.to_numeric(s_tot, errors='coerce').fillna(0).plot(kind='hist', bins=20, ax=ax)
            ax.set_title(f"ICP {thr_tag} – total minutes per patient ({split_label}, {h}h)")
            ax.set_xlabel("total minutes"); plt.tight_layout()
            plt.savefig(os.path.join(out_dir, f"icu_icp_tot_mins_hist_{split_label}_{h}h.png")); plt.close(fig)
        except Exception as e:
            logging.warning(f"[AUDIT] Could not write histograms: {e}")

        # ------------- Episode grid aggregation -------------
        grid_cols = [c for c in cols if c.startswith(f"evtgrid_ICPm_{thr_tag}_count_")]
        grid_rows = []
        for c in grid_cols:
            # c looks like: evtgrid_ICPm_gt25_count_p25_30_d5_15
            tail = c.split("count_")[-1]
            # parse peak bin & dur bin segments
            if "_d" in tail and tail.startswith("p"):
                pseg, dseg = tail.split("_d", 1)
                peak_bin = pseg[1:]   # "25_30" or "lt25" or "ge40"
                dur_bin = dseg        # "5_15" or "lt5" or "ge60"
            else:
                peak_bin, dur_bin = "unknown", "unknown"
            total = pd.to_numeric(icu_df[c], errors='coerce').fillna(0).sum()
            n_with_any = int((pd.to_numeric(icu_df[c], errors='coerce').fillna(0) > 0).sum())
            grid_rows.append({
                "landmark_h": h, "split": split_label, "thr_tag": thr_tag,
                "peak_bin": peak_bin, "dur_bin": dur_bin,
                "total_episodes": int(total),
                "patients_with_any": n_with_any
            })
        if grid_rows:
            df_grid = pd.DataFrame(grid_rows).sort_values(["peak_bin","dur_bin"])
            _write_df_safe(df_grid, os.path.join(out_dir, f"icu_icp_episode_grid_{split_label}_{h}h.csv"))

    else:
        logging.info(f"[AUDIT {h}h/{split_label}] No ICP episode columns found (evt_ICPm_gt*_n_episodes).")

    # ---------------- Joint events (double/triple) ----------------
    joint_rows = []
    for tag in ["doublehit", "triplehit"]:
        jcols = [c for c in cols if c.startswith(f"evt_{tag}_") and c.endswith("_n_episodes")]
        for c in jcols:
            # example: evt_doublehit_icp20_prx030_n_episodes  OR  evt_triplehit_icp20_cpp50_prx030_n_episodes
            spec = c.replace(f"evt_{tag}_", "").replace("_n_episodes", "")
            s = pd.to_numeric(icu_df[c], errors='coerce')
            any_mask = s > 0
            joint_rows.append({
                "landmark_h": h, "split": split_label, "event": tag, "spec": spec,
                "n_patients": n_patients,
                "patients_with_any": int(any_mask.sum()),
                "pct_with_any": float(any_mask.mean()*100.0) if n_patients else np.nan,
                "total_episodes": int(s.fillna(0).sum()),
                **{f"n_episodes_{k}": v for k, v in _summ_stats(s, any_mask).items()}
            })
    if joint_rows:
        df_joint = pd.DataFrame(joint_rows).sort_values(["event","spec"])
        _write_df_safe(df_joint, os.path.join(out_dir, f"icu_joint_episode_summary_{split_label}_{h}h.csv"))

# This function manages the entire process of creating dynamic features for all patients in the study.
# It iterates through each patient one by one, finds their corresponding ICU data files, loads and
# cleans the data, aligns it to their time of injury, and then calls the main feature creation function
# (above) to generate their physiological profile. It then collects all these individual patient profiles
# into a single, master data table that is ready for model training.
def generate_dynamic_features_for_landmark(master_df_landmark: pd.DataFrame, icu_folder_path: str, landmark_h: int, cfg: Dict) -> pd.DataFrame:
    logging.info(f"--- 2. Generating Dynamic Features for Landmark {landmark_h}h ---")
    patient_feature_rows, patient_stats = {}, {}
    pids_to_process = master_df_landmark.index
    fe_cfg = cfg["FEATURE_ENGINEERING"]
    s_id, s_trauma_dt = sanitize_name(COL_ID), sanitize_name(COL_TRAUMA_DATETIME)

    for pid in tqdm(pids_to_process, desc=f"  Processing patients for {landmark_h}h", unit="patient", leave=False):
        icu_paths = sorted(glob(os.path.join(icu_folder_path, f"{pid}*.csv")))
        if not icu_paths:
            patient_stats[pid] = 'no_icu_file'
            continue
        try:
            icu_dfs = [read_icu_table(p) for p in icu_paths]
            df_pat = pd.concat([df for df in icu_dfs if df is not None and not df.empty], ignore_index=True)
            if df_pat.empty:
                patient_stats[pid] = 'empty_icu_file'
                continue

            df_pat = df_pat.copy(deep=True)

            df_pat.columns = [sanitize_name(c) for c in df_pat.columns]
            df_pat = ensure_icu_timestamp(df_pat)
            s_icu_time = sanitize_name(COL_ICU_TIME)
            if s_icu_time not in df_pat.columns or df_pat[s_icu_time].isna().all():
                patient_stats[pid] = 'no_timestamps'
                continue

            trauma_dt = master_df_landmark.loc[pid, s_trauma_dt]
            if pd.isna(trauma_dt):
                patient_stats[pid] = 'no_trauma_datetime'
                continue
            df_pat[COL_TIME_AFTER_TRAUMA] = df_pat[s_icu_time] - trauma_dt

            # Cut to [0, H]
            df_pat_h = df_pat[(df_pat[COL_TIME_AFTER_TRAUMA] >= pd.Timedelta(0)) & (df_pat[COL_TIME_AFTER_TRAUMA] <= pd.to_timedelta(landmark_h, 'h'))].copy()
            df_pat_h = df_pat_h.set_index(COL_TIME_AFTER_TRAUMA).sort_index()
            if df_pat_h.empty:
                patient_stats[pid] = 'no_data_in_window'
                continue
            # Clean implausible values
            df_pat_h = filter_plausible_icu_values(df_pat_h, fe_cfg["PLAUSIBLE_RANGES"])

            # Identify dynamic variables we actually have
            dyn_vars = [sanitize_name(v) for v in fe_cfg["DYNAMIC_VARS"] if sanitize_name(v) in df_pat_h.columns]
            if not dyn_vars:
                patient_stats[pid] = 'no_dynamic_vars'
                continue

            if df_pat_h[dyn_vars].notna().sum().sum() > 0:
                patient_features = create_dynamic_features_from_patient_data(df_pat_h, landmark_h, cfg)
                if not patient_features.empty:
                    patient_feature_rows[pid] = patient_features
                    patient_stats[pid] = 'success'
                else: patient_stats[pid] = 'not_enough_clean_data'
            else: patient_stats[pid] = 'no_valid_data_after_clean'
        except Exception as e:
            logging.warning(f"Failed to process ICU data for patient {pid}: {e}", exc_info=False)
            patient_stats[pid] = 'processing_error'

    stats_counts = pd.Series(patient_stats).value_counts()
    logging.info(f"ICU processing stats for {landmark_h}h landmark: \n{stats_counts.to_string()}")

    try:
        audit_path = os.path.join(cfg["RUN_INFO"]["OUTPUT_FOLDER"], f"icu_processing_stats_{landmark_h}h.csv")
        pd.Series(patient_stats, name='status').to_csv(audit_path, header=True)
    except Exception as e:
        logging.warning(f"Could not save ICU processing stats for {landmark_h}h: {e}")

    if not patient_feature_rows:
        logging.warning(f"No dynamic features could be generated for the {landmark_h}h landmark.")
        return pd.DataFrame()

    dynamic_features_df = pd.DataFrame.from_dict(patient_feature_rows, orient='index')
    dynamic_features_df.index.name = s_id
    logging.info(f"Successfully generated dynamic features for {len(dynamic_features_df)} patients at {landmark_h}h.")
    return dynamic_features_df

# This function intelligently selects the most important ICU features to use in our final predictive model.
# It now uses a powerful technique called Recursive Feature Elimination (RFE) for this task. RFE works
# by repeatedly building a model, removing the weakest features, and rebuilding the model until only
# the strongest, most predictive features remain. This helps the model focus on what matters most.
def select_features_for_icu_model(
    X_baseline: pd.DataFrame,
    X_icu: pd.DataFrame,
    y: pd.Series,
    cfg: Dict
) -> list:
    fs_cfg = cfg["FEATURE_SELECTION"]
    top_n_icu = int(fs_cfg["TOP_N_ICU_FEATURES"])  # consider increasing to 40–60 in CONFIG
    rfe_step = fs_cfg.get("RFE_STEP", 0.1)


    if X_icu.empty:
        logging.warning("ICU feature matrix is empty; selecting baseline only.")
        return X_baseline.columns.tolist()


    X_icu_num = X_icu.select_dtypes(include=np.number)
    if X_icu_num.shape[1] == 0:
        logging.warning("No numeric ICU features; selecting baseline only.")
        return X_baseline.columns.tolist()


    # Relaxed coverage: >= 40% non-NaN
    coverage = X_icu_num.notna().mean(axis=0)
    keep_cols = coverage[coverage >= 0.40].index.tolist()
    if not keep_cols:
        logging.warning("No ICU features pass coverage >=40%; falling back to baseline only.")
        return X_baseline.columns.tolist()
    logging.info(f"Coverage filter kept {len(keep_cols)}/{X_icu_num.shape[1]} ICU features for RFE (>=40% non-NaN).")
    X_icu_num = X_icu_num[keep_cols]
    n_candidates = X_icu_num.shape[1]
    if n_candidates == 0:
        logging.warning("No ICU candidates after coverage; selecting baseline only.")
        return X_baseline.columns.tolist()

    n_select = min(top_n_icu, n_candidates)
    if n_select < 1:
        logging.warning("No ICU features to select; selecting baseline only.")
        return X_baseline.columns.tolist()


    y_aligned = y.loc[X_icu_num.index].dropna().astype(int)
    X_icu_num = X_icu_num.loc[y_aligned.index]


    if X_icu_num.empty:
        logging.warning("ICU selection: no usable rows after alignment; selecting baseline only.")
        return X_baseline.columns.tolist()


    n_classes = int(pd.Series(y_aligned).nunique())

    selector_model = xgb.XGBClassifier(
        n_estimators=50,
        max_depth=3,
        learning_rate=0.1,
        random_state=cfg["MODELING"]["RANDOM_STATE"],
        n_jobs=cfg["MODELING"]["N_JOBS"],
        tree_method='hist',
        objective='multi:softprob',
        num_class=n_classes
    )


    rfe = RFE(
        estimator=selector_model,
        n_features_to_select=n_select,
        step=rfe_step,
        verbose=1
    )


    num_feats = X_icu_num.columns.tolist()
    pre = ColumnTransformer(
        transformers=[('num', SimpleImputer(strategy='median'), num_feats)],
        remainder='drop',
        verbose_feature_names_out=False
    )
    pipe = Pipeline([('preprocessor', pre), ('selector', rfe)])


    logging.info(f"Starting RFE to select {top_n_icu} features from {X_icu_num.shape[1]} candidates.")
    pipe.fit(X_icu_num, y_aligned)


    support_mask = pipe.named_steps['selector'].support_
    top_icu_cols = X_icu_num.columns[support_mask].tolist()
    logging.info(f"RFE selected {len(top_icu_cols)} ICU features: {top_icu_cols[:5]}...")


    return X_baseline.columns.tolist() + top_icu_cols

def build_icu_feature_subsets(icu_df: pd.DataFrame) -> Dict[str, List[str]]:
    """
    Returns column name lists for ICU ablations:
    - 'icp_only': ICP-derived features only (no CPP/PRx/joint).
    - 'icp_cpp': ICP + CPP features, including ICP-CPP 'doublehit', excluding PRx.
    - 'prx_only': PRx-derived features only (no ICP/CPP/joint).
    """
    if icu_df is None or icu_df.empty:
        return {"icp_only": [], "icp_cpp": [], "prx_only": []}


    cols = icu_df.columns.tolist()
    lc = {c: c.lower() for c in cols}


    def any_tokens(c: str, tokens: List[str]) -> bool:
        return any(t in lc[c] for t in tokens)


    def none_tokens(c: str, tokens: List[str]) -> bool:
        return all(t not in lc[c] for t in tokens)
    
    exclude_cppopt = ['cppopt', 'dcppopt']
    bad_joint = ['triplehit', 'red_'] # joint event names
    prx_tokens = ['prx']
    icp_tokens = ['icpm', 'icp']
    cpp_tokens = ['cpp']


    icp_only = [c for c in cols
        if any_tokens(c, icp_tokens)
        and none_tokens(c, cpp_tokens + prx_tokens + bad_joint)]
    # include relevant meta flags
    icp_only += [c for c in cols if lc[c].startswith('meta_has_') and any_tokens(c, icp_tokens)]

    
    icp_cpp = [c for c in cols
    if (
        any_tokens(c, icp_tokens)
        or (any_tokens(c, cpp_tokens) and none_tokens(c, exclude_cppopt))
        or ('evt_doublehit' in lc[c] and '_cpp' in lc[c])   # include ICP–CPP doublehit
    )
    and none_tokens(c, prx_tokens)                          # exclude PRx features entirely
    and ('evt_triplehit' not in lc[c])                      # exclude triplehit (has PRx)
]

    prx_only = [c for c in cols
        if any_tokens(c, prx_tokens)
        and none_tokens(c, icp_tokens + cpp_tokens + bad_joint)]
    prx_only += [c for c in cols if lc[c].startswith('meta_has_') and any_tokens(c, prx_tokens)]


    def dedupe(seq: List[str]) -> List[str]:
        seen: set = set(); out: List[str] = []
        for x in seq:
            if x not in seen:
                seen.add(x); out.append(x)
        return out


    return {"icp_only": dedupe(icp_only), "icp_cpp": dedupe(icp_cpp), "prx_only": dedupe(prx_only)}

def calculate_calibration_metrics(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> Dict[str, float]:
    y_true = np.asarray(y_true).astype(int)
    y_prob = np.clip(np.asarray(y_prob), 1e-6, 1 - 1e-6)  # avoid inf logits

    metrics = {"ece": np.nan, "cal_in_large": np.nan, "cal_slope": np.nan}
    if len(np.unique(y_true)) < 2:
        logging.warning("Calibration metrics skipped: only one class present in y_true.")
        return metrics

    # Logistic calibration model: logit(y) ~ a + b*logit(p̂)
    logit_p = np.log(y_prob / (1 - y_prob)).reshape(-1, 1)
    try:
        lr = LogisticRegression(solver='lbfgs', C=1e6, fit_intercept=True)
        lr.fit(logit_p, y_true)
        metrics["cal_slope"] = float(lr.coef_[0][0])
        metrics["cal_in_large"] = float(lr.intercept_[0])  # CITL on log-odds scale
    except Exception as e:
        logging.warning(f"Calibration model failed: {e}")

    # ECE (equal-width bins)
    try:
        bins = np.linspace(0.0, 1.0, n_bins + 1)
        bin_ids = np.digitize(y_prob, bins[1:-1])
        ece = 0.0
        for i in range(n_bins):
            in_bin = (bin_ids == i)
            n_in = in_bin.sum()
            if n_in:
                acc = y_true[in_bin].mean()
                conf = y_prob[in_bin].mean()
                ece += n_in * abs(acc - conf)
        metrics["ece"] = ece / len(y_prob)
    except Exception as e:
        logging.warning(f"Could not calculate ECE: {e}")

    return metrics


# ============================== PUBLICATION TABLES =======================================
def generate_publication_tables(
    cfg: Dict,
    cohort_data: Dict,
    master_train_df: pd.DataFrame,
    master_test_df: pd.DataFrame,
    flow_log: List[Tuple[str, int]]
):
    """
    Generates and saves publication-ready tables.
    This version uses a `flow_log` and correctly reports on the cleaned CT findings.
    """
    logging.info("--- Generating Publication Tables (Flowchart, Demographics, Distributions) ---")
    output_path = os.path.join(cfg["RUN_INFO"]["OUTPUT_FOLDER"], "Publication_Tables.xlsx")

    analysis_ready_df = pd.concat([master_train_df, master_test_df])
    s_id = sanitize_name(COL_ID)
    if analysis_ready_df.index.name == s_id:
        analysis_ready_df = analysis_ready_df.reset_index()

    # ----------------------------- ALL HELPERS DEFINED FIRST -----------------------------
    
    def _ensure_gcs_sum(df: pd.DataFrame) -> str:
        # (helper function is unchanged)
        s_eye, s_verbal, s_motor = map(sanitize_name, [COL_GCS_EYE_SCORE, COL_GCS_VERBAL_SCORE, COL_GCS_MOTOR_SCORE])
        s_sum = sanitize_name(COL_GCS_SUM)
        if s_sum in df.columns:
            df[s_sum] = pd.to_numeric(df[s_sum], errors='coerce')
            return s_sum
        have_scores = [c for c in [s_eye, s_verbal, s_motor] if c in df.columns]
        if have_scores:
            for c in have_scores: df[c] = pd.to_numeric(df[c], errors='coerce')
            df[s_sum] = df[have_scores].sum(axis=1, min_count=1)
            return s_sum
        return ""

    def _panel_a(log: List[Tuple[str, int]]) -> pd.DataFrame:
        return pd.DataFrame(log, columns=["Criterion", "Count"])

    def _attach_death_flags(df_in: pd.DataFrame) -> pd.DataFrame:
        df = df_in.copy()
        s_trauma_dt, s_death_dt = sanitize_name(COL_TRAUMA_DATETIME), sanitize_name(COL_DATE_OF_DEATH)
        df[s_trauma_dt] = pd.to_datetime(df.get(s_trauma_dt), errors='coerce')
        df[s_death_dt] = pd.to_datetime(df.get(s_death_dt), errors='coerce')
        for h in cfg["FEATURE_ENGINEERING"]["PREDICTION_LANDMARKS_HOURS"]:
            col = f'Died_within_{int(h)}h'
            if s_trauma_dt in df.columns and s_death_dt in df.columns:
                died_flag = (df[s_death_dt] - df[s_trauma_dt]) <= pd.to_timedelta(int(h), 'h')
                df[col] = died_flag.fillna(False)
            else:
                df[col] = False
        return df

    def _panel_b(df_after_A_truth: pd.DataFrame, landmark_h: int, cohort_data: Dict) -> Tuple[pd.DataFrame, set]:
        rows, start_ids = [], set(df_after_A_truth[s_id].astype(str))
        rows.append(("Carryover from Panel A (Analysis-Ready Cohort)", len(start_ids)))
        death_flag_col = f'Died_within_{landmark_h}h'
        died_mask = df_after_A_truth.get(death_flag_col, pd.Series(False, index=df_after_A_truth.index))
        num_died = int(died_mask.sum())
        df_alive = df_after_A_truth[~died_mask]
        alive_ids = set(df_alive[s_id].astype(str))
        rows.append((f"  — Excluded: died before {landmark_h}h", num_died))
        rows.append((f"Patients alive at {landmark_h}h", len(alive_ids)))
        h2h_key = f"h2h_{landmark_h}h_ids"
        ids_h2h = {str(x) for x in cohort_data.get(h2h_key, set())}
        final_ids = alive_ids & ids_h2h
        rows.append((f"  — Excluded: discharged or no valid ICU data before {landmark_h}h", len(alive_ids) - len(final_ids)))
        rows.append((f"Final analysis cohort @ {landmark_h}h", len(final_ids)))
        return pd.DataFrame(rows, columns=["Criterion", "Patients Remaining"]), final_ids

    def _ensure_gose_numeric(df: pd.DataFrame) -> str:
        s_out = sanitize_name(COL_OUTCOME)
        if s_out in df.columns:
            cleaned = (df[s_out].astype(str).str.lower().str.strip().replace(r'[^a-z0-9\s-]', '', regex=True))
            df["GOSE_1_8_temp"] = cleaned.map(GOSE_TEXT_TO_NUMERIC_MAP)
            return "GOSE_1_8_temp"
        return COL_TARGET_GOSE_1_8

    def _build_demographics_table(df_cohort: pd.DataFrame) -> pd.DataFrame:
        demographics_data = []
        total_patients = int(len(df_cohort))

        def _add_stat(group, characteristic, value):
            demographics_data.append({"Group": group, "Characteristic": characteristic, "Value": value})

        def _add_categorical_summary(df, col_name, group_name, characteristic_name):
            s_col = sanitize_name(col_name)
            if s_col not in df.columns: return
            counts = df[s_col].value_counts(dropna=False)
            for category, count in counts.items():
                percent = (count / max(total_patients, 1)) * 100
                cat_label = "Missing/Unknown" if pd.isna(category) else str(category)
                _add_stat(group_name, f"{characteristic_name}: {cat_label}", f"{int(count)} ({percent:.1f}%)")

        def _add_binary_summary(df, col_name, group_name):
            s_col = sanitize_name(col_name)
            if s_col not in df.columns: return
            series = pd.to_numeric(df[s_col].replace({r'(?i)^\s*(ja|yes|true|1|1\.0)\s*$': 1, r'(?i)^\s*(nej|no|false|0|0\.0)\s*$': 0}, regex=True), errors='coerce')
            count = int(series.fillna(0).sum())
            percent = (count / max(total_patients, 1)) * 100
            _add_stat(group_name, col_name, f"{count} ({percent:.1f}%)")

        # --- Build Table ---
        _add_categorical_summary(df_cohort, "Gender", "Admission data", "Sex")
        age_col = sanitize_name("Age")
        if age_col in df_cohort.columns:
            age = pd.to_numeric(df_cohort[age_col], errors='coerce').dropna()
            if len(age):
                _add_stat("Admission data", "Mean age (SD)", f"{age.mean():.1f} ({age.std():.1f})")
                _add_stat("Admission data", "Median age (IQR)", f"{age.median():.0f} ({age.quantile(0.25):.0f}-{age.quantile(0.75):.0f})")

        _add_binary_summary(df_cohort, "Hypertension/Cardiovascular disease", "Comorbidities")
        _add_binary_summary(df_cohort, "Diabetes", "Comorbidities")
        _add_binary_summary(df_cohort, "Alcoholism", "Comorbidities")
        _add_categorical_summary(df_cohort, "Mechanism of injury", "Trauma mechanism", "Type")
        
        ### SECTION for Radiology ###
        _add_categorical_summary(df_cohort, COL_CT_FINDINGS, "Radiology findings (CT)", "CT Finding")
        _add_categorical_summary(df_cohort, COL_MARSHALL_SCORE, "Radiology findings (CT)", "Marshall score")
        _add_binary_summary(df_cohort, "Intrakraniell_hematom-utrymning", "Radiology findings (CT)")
        
        s_gcs_motor_score = sanitize_name(COL_GCS_MOTOR_SCORE)
        if s_gcs_motor_score in df_cohort.columns:
            gcs_counts = df_cohort[s_gcs_motor_score].value_counts().sort_index()
            for score, count in gcs_counts.items():
                percent = (count / max(total_patients, 1)) * 100
                _add_stat("GCS-Motor", f"Score {int(score)}", f"{int(count)} ({percent:.1f}%)")

        s_pupil_l, s_pupil_r = sanitize_name(COL_PUPIL_L_REACT), sanitize_name(COL_PUPIL_R_REACT)
        if s_pupil_l in df_cohort.columns and s_pupil_r in df_cohort.columns:
            df_cohort['_pupil_l_bin'] = pd.to_numeric(df_cohort[s_pupil_l], errors='coerce').fillna(-1)
            df_cohort['_pupil_r_bin'] = pd.to_numeric(df_cohort[s_pupil_r], errors='coerce').fillna(-1)
            both = int(((df_cohort['_pupil_l_bin'] == 1) & (df_cohort['_pupil_r_bin'] == 1)).sum())
            one = int((((df_cohort['_pupil_l_bin'] == 1) & (df_cohort['_pupil_r_bin'] == 0)) | ((df_cohort['_pupil_l_bin'] == 0) & (df_cohort['_pupil_r_bin'] == 1))).sum())
            none = int(((df_cohort['_pupil_l_bin'] == 0) & (df_cohort['_pupil_r_bin'] == 0)).sum())
            denom = max(total_patients, 1)
            _add_stat("Pupil reactivity", "Both pupils reacting", f"{both} ({(both / denom) * 100:.1f}%)")
            _add_stat("Pupil reactivity", "One pupil reacting", f"{one} ({(one / denom) * 100:.1f}%)")
            _add_stat("Pupil reactivity", "No pupil reacting", f"{none} ({(none / denom) * 100:.1f}%)")

        return pd.DataFrame(demographics_data)

    def _long_distribution(series: pd.Series, allowed_values: list, label_name: str, landmark_h: int) -> pd.DataFrame:
        s = pd.to_numeric(series, errors='coerce').dropna().astype(int)
        n = int(len(s))
        rows = []
        for v in allowed_values:
            cnt = int((s == v).sum()) if n else 0
            pct = (cnt / n * 100.0) if n else 0.0
            rows.append({"Landmark (h)": int(landmark_h), label_name: int(v), "Count": cnt, "Percent": round(pct, 1), "N": n})
        return pd.DataFrame(rows)

    # ----------------------------- Main Logic Starts Here -----------------------------
    
    df_flow_A = _panel_a(flow_log)
    df_for_downstream = _attach_death_flags(analysis_ready_df)
    
    landmarks = list(map(int, cfg["FEATURE_ENGINEERING"]["PREDICTION_LANDMARKS_HOURS"]))
    panel_b_tables, final_ids_by_h = {}, {}
    for h in landmarks:
        df_flow_B, final_ids = _panel_b(df_for_downstream, h, cohort_data)
        panel_b_tables[f"Flowchart Panel B ({h}h)"] = df_flow_B
        final_ids_by_h[h] = final_ids

    gose_col = _ensure_gose_numeric(df_for_downstream)
    gcs_sum_col = _ensure_gcs_sum(df_for_downstream)

    demo_sheets, gose_dist_rows, gcs_dist_rows = {}, [], []
    for h, idset in final_ids_by_h.items():
        if not idset: continue
        df_cohort_h = df_for_downstream[df_for_downstream[s_id].astype(str).isin({str(x) for x in idset})].copy()
        demo_sheets[f"Demographics @ {h}h"] = _build_demographics_table(df_cohort_h)
        if gose_col and gose_col in df_cohort_h.columns:
            gose_dist_rows.append(_long_distribution(df_cohort_h[gose_col], list(range(1, 9)), "GOSE (1–8)", h))
        if gcs_sum_col and gcs_sum_col in df_cohort_h.columns:
            gcs_dist_rows.append(_long_distribution(df_cohort_h[gcs_sum_col], list(range(3, 16)), "GCS Sum (3–15)", h))

    df_gose_dist = pd.concat(gose_dist_rows, ignore_index=True) if gose_dist_rows else pd.DataFrame()
    df_gcs_dist = pd.concat(gcs_dist_rows, ignore_index=True) if gcs_dist_rows else pd.DataFrame()

    # --- Write to Excel ---
    try:
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            sheet_map = {
                "Patient Flowchart Panel A": df_flow_A,
                **panel_b_tables,
                **demo_sheets
            }
            if not df_gose_dist.empty: sheet_map["GOSE Distribution"] = df_gose_dist
            if not df_gcs_dist.empty: sheet_map["GCS Sum Distribution"] = df_gcs_dist
            for sheet_name, df_ in sheet_map.items():
                df_.to_excel(writer, sheet_name=sheet_name, index=False)
                ws = writer.sheets[sheet_name]
                for i, col in enumerate(df_.columns):
                    max_len = max(df_[col].astype(str).map(len).max() if len(df_) else 0, len(str(col))) + 2
                    ws.set_column(i, i, max_len)
        logging.info(f"Successfully generated publication tables → {output_path}")
    except Exception as e:
        logging.error(f"Failed to write publication tables: {e}", exc_info=True)

# ============================== MODELING & EVALUATION ====================================
# This function is a crucial data preparation step that gets the data ready for the machine learning model.
# It handles numeric and categorical variables differently to ensure they are in a format the model can understand.
def get_preprocessor(numeric_feats, cat_feats, for_tree=False):
    ver = tuple(int(x) for x in sklearn.__version__.split('.')[:2])
    ohe_kwargs = {'handle_unknown': 'ignore'}
    if ver >= (1, 2):
        ohe_kwargs['sparse_output'] = False
    else:
        ohe_kwargs['sparse'] = False
    if ver >= (1, 1):
        ohe_kwargs['min_frequency'] = 0.03

    if for_tree:
        numeric_pipe = 'passthrough'
    else:
        numeric_pipe = Pipeline([
            ('imputer', SimpleImputer(strategy='median', add_indicator=True)),
            ('scaler', StandardScaler())
        ])


    cat_pipe = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='__MISSING__')),
        ('ohe', OneHotEncoder(**ohe_kwargs))
    ])

    preprocessor = ColumnTransformer(
        [('num', numeric_pipe, numeric_feats), ('cat', cat_pipe, cat_feats)],
        remainder='drop', verbose_feature_names_out=False
    )
    return preprocessor

# This function trains and optimizes a machine learning model. Think of it like tuning a microscope to get the
# clearest possible image. The function uses an automated process to test many different settings for the model
# to find the combination that gives the best performance.
def fit_and_tune_model(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    model_name: str,
    cfg: Dict,
    time_order: Optional[pd.Series] = None
) -> Tuple[BaseEstimator, float]:

    logging.info(f"Tuning model: {model_name}")

    # Clean columns
    X_train = X_train.dropna(axis=1, how='all')
    num_feats = X_train.select_dtypes(include=np.number).columns.tolist()
    cat_feats = X_train.select_dtypes(exclude=np.number).columns.tolist()
    pre = get_preprocessor(num_feats, cat_feats, for_tree=(model_name == MODEL_XGB))

    # ---------------- Model-specific pipeline ----------------
    if model_name == MODEL_XGB:
        clf = xgb.XGBClassifier(
            random_state=cfg["MODELING"]["RANDOM_STATE"],
            eval_metric='mlogloss',
            objective='multi:softprob',
            tree_method='hist'
        )
        pipe = Pipeline([('preprocessor', pre), ('classifier', clf)])
        y_fit = y_train.dropna().astype(int)
        X_train_fit = X_train.loc[y_fit.index]

        # ----- class weights -----
        fit_params: Dict[str, Any] = {}
        cw_mode = cfg["MODELING"].get("CLASS_WEIGHTS", "balanced")
        if cw_mode == "balanced":
            cls_w = compute_sample_weight(class_weight="balanced", y=y_fit)
            exp = float(cfg["MODELING"].get("CLASS_WEIGHT_EXPONENT", 1.0))
            if exp != 1.0:
                cls_w = cls_w ** exp
            fit_params = {"classifier__sample_weight": cls_w}
        elif cw_mode == "none" or cw_mode is None:
            fit_params = {}
        elif isinstance(cw_mode, dict):
            # allow int or str keys in the dict
            def _lookup(c):
                return cw_mode.get(c, cw_mode.get(str(c), 1.0))
            w = y_fit.map(_lookup).to_numpy()
            fit_params = {"classifier__sample_weight": w}
        else:
            logging.warning(f"Unrecognized CLASS_WEIGHTS={cw_mode!r}; proceeding with no weights.")
            fit_params = {}

        scoring_metric = _qwk_scorer_basic
        logging.info(" [XGBoost] Optimizing hyperparameters for Quadratic Weighted Kappa (QWK).")

    elif model_name == MODEL_ORDINAL and MORD_AVAILABLE:
        # mord expects labels 1..K
        y_fit = y_train.dropna().astype(int) + 1
        X_train_fit = X_train.loc[y_fit.index]
        pipe = Pipeline([('preprocessor', pre), ('classifier', mord.LogisticAT(alpha=1.0))])
        fit_params = {}
        scoring_metric = _qwk_scorer_basic
        logging.info(f" [{model_name}] Optimizing hyperparameters for Quadratic Weighted Kappa.")

    else:
        return None, -1

    # ---------------- Cross-validation scheme ----------------
    grid = cfg["MODELING"]["PARAM_GRIDS"].get(model_name, {})
    cv_type = cfg["MODELING"].get("CV_TYPE", "stratified").lower()

    if cv_type == "timeseries":
        if time_order is None:
            raise ValueError("CV_TYPE='timeseries' but time_order is None. Pass a pd.Series of datetimes aligned to X_train.")
        n_samples = len(X_train_fit)
        n_splits = 5 # Reduced from 10 to ensure larger initial folds
        
        # Set a maximum training size to create a "sliding window".
        # This prevents very old data from overly influencing the model.
        # Here, we'll use 80% of the data as the max window size.
        max_train_size = int(n_samples * 0.8)
        
        logging.info(f"Using TimeSeriesSplit with {n_splits} splits and max_train_size={max_train_size}.")

        order = pd.to_datetime(time_order.loc[X_train_fit.index], errors="coerce")
        valid = order.notna()
        if not valid.all():
            logging.warning(f"Dropping {int((~valid).sum())} rows with invalid time order for TimeSeries CV.")
        
        X_train_fit = X_train_fit.loc[valid]
        y_fit = y_fit.loc[valid]
        sort_idx = order.loc[valid].sort_values().index
        X_train_fit = X_train_fit.loc[sort_idx]
        y_fit = y_fit.loc[sort_idx]
        
        cv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)
    else:
        cv = StratifiedKFold(
            n_splits=cfg["MODELING"]["CV_SPLITS"],
            shuffle=True,
            random_state=cfg["MODELING"]["RANDOM_STATE"]
        )
        logging.info(f"Using StratifiedKFold with {cfg['MODELING']['CV_SPLITS']} splits.")

    # ---------------- Hyperparameter search ----------------
    if grid:
        search = RandomizedSearchCV(
            pipe, grid,
            n_iter=cfg["MODELING"]["HYPERPARAM_N_ITER"],
            cv=cv,
            scoring=scoring_metric,
            n_jobs=cfg["MODELING"]["N_JOBS"],
            random_state=cfg["MODELING"]["RANDOM_STATE"],
            refit=True,
            error_score=0.0
        )
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.impute")
            search.fit(X_train_fit, y_fit, **fit_params)
        best_pipe = search.best_estimator_
        best_score = search.best_score_
    else:
        logging.info("No hyperparameter grid provided; fitting base pipeline without tuning.")
        best_pipe = pipe.fit(X_train_fit, y_fit, **fit_params)
        best_score = float('nan')

    # ---------------- Post-fit cumulative thresholds ----------------
    try:
        clf_step = best_pipe.named_steps.get('classifier', None)
        if hasattr(clf_step, 'predict_proba'):
            mode = str(cfg["MODELING"].get("THRESHOLD_MODE", "qwk")).lower()
            prior_w = float(cfg["MODELING"].get("PRIOR_WEIGHT", 0.25))

            # y_fit_final should be 0..6 for both models before passing in
            y_fit_final = y_fit.copy()
            if MORD_AVAILABLE and isinstance(clf_step, (mord.LogisticAT, getattr(mord, 'LogisticIT', tuple()))):
                if y_fit_final.min() == 1:
                    y_fit_final = y_fit_final - 1
            
            tau_qwk_result = fit_qwk_cum_thresholds(best_pipe, X_train_fit, y_fit_final)
            tau_qwk = tau_qwk_result if tau_qwk_result is not None else np.full(6, 0.5)

            tau_prior_result = fit_prior_matched_cum_thresholds(best_pipe, X_train_fit, y_fit_final)
            tau_prior = tau_prior_result if tau_prior_result is not None else np.full(6, 0.5)

            if mode == "qwk":
                tau = tau_qwk
            elif mode == "prior":
                tau = tau_prior
            else:  # "hybrid"
                tau = (1.0 - prior_w) * tau_qwk + prior_w * tau_prior
                tau = np.minimum.accumulate(tau)  # ensure monotone

            best_pipe.qwk_thresholds_ = tau
            logging.info(f"Stored thresholds (mode={mode}, prior_w={prior_w}): {np.round(tau, 3).tolist()}")
    except Exception as e:
        logging.warning(f"Could not set thresholds: {e}")

    # Diagnostics
    try:
        # normalize counts to 0..6 for logging (mord case)
        y_log = y_fit if (y_fit.min() == 0) else (y_fit - 1)
        logging.info(f"[Train label counts] {pd.Series(y_log).value_counts().sort_index().to_dict()}")
    except Exception:
        pass

    return best_pipe, best_score

# This is the final and most important step, where we assess how well the fully trained model performs on a
# set of patients it has never seen before (the test set). It calculates all the key performance metrics
# and generates the figures and tables you would typically see in a clinical research paper.
def evaluate_and_save_artifacts(final_pipeline, X_test, y_test, model_info, X_train, cfg):
    """
    Evaluates a trained pipeline on test data and saves all relevant artifacts
    with enhanced, publication-quality figures.
    """
    folder = cfg["RUN_INFO"]["OUTPUT_FOLDER"]
    model_folder = os.path.join(
        folder,
        f"{model_info['h']}h_{model_info['comparison_group']}_{model_info['fs_name']}_{model_info['model_name']}"
    )
    os.makedirs(model_folder, exist_ok=True)

    logging.info(
        f"\n{'='*20} FINAL TEST EVALUATION FOR: {model_info['model_name']} "
        f"({model_info['fs_name']} @ {model_info['h']}h on '{model_info['comparison_group']}' cohort) {'='*20}"
    )

    # --- Prepare Data for Evaluation ---
    valid_outcome_mask = y_test[COL_TARGET_ORDINAL_0_6].notna()
    X_test_eval = X_test.loc[valid_outcome_mask]
    y_test_eval = y_test.loc[valid_outcome_mask]

    if X_test_eval.empty:
        logging.warning("No test samples with valid outcomes. Skipping evaluation.")
        return

    # --- Ordinal Prediction & Core Metric (QWK) ---
    y_pred_ordinal = predict_with_qwk_thresholds(final_pipeline, X_test_eval)
    y_true_ordinal = y_test_eval[COL_TARGET_ORDINAL_0_6].astype(int).values

    qwk, qwk_low, qwk_high = bootstrap_metric(
        y_true_ordinal, y_pred_ordinal, cohen_kappa_score,
        weights='quadratic', random_state=cfg["MODELING"]["RANDOM_STATE"]
    )
    logging.info(f"==> Final Test QWK: {qwk:.3f} (95% CI: {qwk_low:.3f} - {qwk_high:.3f})")

    # --- Probabilistic Metrics (AUROC, Calibration) ---
    classifier = final_pipeline.named_steps['classifier']
    if hasattr(classifier, "predict_proba"):
        try:
            y_pred_proba_all = final_pipeline.predict_proba(X_test_eval)
            classes = getattr(classifier, "classes_", np.arange(y_pred_proba_all.shape[1]))

            # mord sometimes yields classes 1..K; normalize to 0..K-1
            if np.issubdtype(classes.dtype, np.integer) and classes.min() == 1:
                classes = classes - 1

            cls_to_idx = {int(c): i for i, c in enumerate(classes)}

            # ----- Death / VS (class 0 or GOSE 1–2 merged) -----
            dead_or_vs_idx = cls_to_idx.get(0)
            if dead_or_vs_idx is not None:
                y_prob_dead_or_vs = y_pred_proba_all[:, dead_or_vs_idx]
                y_true_dead_or_vs = (y_test_eval[COL_TARGET_GOSE_1_8].isin([1, 2])).astype(int).values
                if len(np.unique(y_true_dead_or_vs)) > 1:
                    auroc_dv, lo, hi = bootstrap_metric(
                        y_true_dead_or_vs, y_prob_dead_or_vs, roc_auc_score,
                        random_state=cfg["MODELING"]["RANDOM_STATE"]
                    )
                    logging.info(f"==> Death/VS AUROC: {auroc_dv:.3f} (95% CI: {lo:.3f} - {hi:.3f})")
                else:
                     logging.warning("Only one class present for Death/VS outcome; AUROC cannot be calculated.")

            # ----- Favorable outcome (GOSE >=5 → ordinal >=3) -----
            favorable_indices = [cls_to_idx[c] for c in classes if int(c) >= 3 and c in cls_to_idx]
            if favorable_indices:
                y_prob_favorable = y_pred_proba_all[:, favorable_indices].sum(axis=1)
                y_true_favorable = y_test_eval[COL_TARGET_FAVORABLE].values
                if len(np.unique(y_true_favorable)) > 1:
                    auroc_f, auroc_f_low, auroc_f_high = bootstrap_metric(
                        y_true_favorable, y_prob_favorable, roc_auc_score,
                        random_state=cfg["MODELING"]["RANDOM_STATE"]
                    )
                    logging.info(
                        f"==> {COL_TARGET_FAVORABLE} AUROC: {auroc_f:.3f} "
                        f"(95% CI: {auroc_f_low:.3f} - {auroc_f_high:.3f})"
                    )
                else:
                    logging.warning("Only one class present for Favorable outcome; AUROC cannot be calculated.")


            # ===== Per-category calibration plot =====
            y_true_cat = y_test_eval[COL_TARGET_ORDINAL_0_6].astype(int).values
            order = sorted(cls_to_idx.keys())
            P = y_pred_proba_all[:, [cls_to_idx[c] for c in order]]
            n_classes = P.shape[1]

            ncols = 4
            nrows = int(np.ceil(n_classes / ncols))
            fig, axes = plt.subplots(nrows, ncols, figsize=(4.0*ncols, 3.5*nrows), squeeze=False)
            axes = axes.flatten()

            for k in range(n_classes):
                yk = (y_true_cat == k).astype(int)
                pk = P[:, k]
                title_k = cfg["ARTIFACTS"]["GOSE_ORDINAL_LABELS"].get(k, f"Class {k}")
                _plot_cal_panel(axes[k], yk, pk, title=title_k, n_bins=20, strategy="quantile")

            for j in range(n_classes, len(axes)):
                axes[j].axis("off")
            
            fig.suptitle(
                f"Calibration by Outcome Category (1-vs-Rest)\n"
                f"{model_info['model_name']} | {model_info['fs_name']} @ {model_info['h']}h",
                fontsize=16
            )
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.savefig(os.path.join(model_folder, "final_calibration_by_category.png"), dpi=300)
            plt.close(fig)

        except Exception as e:
            logging.warning("Could not calculate binary metrics or probability plots: %s", e, exc_info=True)

    # --- Confusion Matrix (Counts) ---
    fig, ax = plt.subplots(figsize=(10, 8))
    labels_sorted = sorted(np.unique(np.concatenate((y_true_ordinal, y_pred_ordinal))))
    display_labels = [cfg["ARTIFACTS"]["GOSE_ORDINAL_LABELS"].get(k, str(k)) for k in labels_sorted]

    ConfusionMatrixDisplay.from_predictions(
        y_true_ordinal, y_pred_ordinal,
        labels=labels_sorted, display_labels=display_labels,
        cmap=plt.cm.Blues, normalize=None, ax=ax, xticks_rotation='vertical'
    )

    ax.set_xticks(range(len(labels_sorted)))
    ax.set_xticklabels(display_labels, rotation=45, ha="right")
    ax.set_yticks(range(len(labels_sorted)))
    ax.set_yticklabels(display_labels)

    ax.set_xlabel("Predicted Outcome", fontsize=12)
    ax.set_ylabel("Observed Outcome", fontsize=12)

    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

    ax.set_title(
        f"Confusion Matrix (Patient Counts) – Test Set\n"
        f"{model_info['model_name']} | {model_info['fs_name']} @ {model_info['h']}h | QWK = {qwk:.3f}",
        fontsize=14
    )
    plt.tight_layout()
    plt.savefig(os.path.join(model_folder, "final_confusion_matrix_counts.png"), dpi=300)
    plt.close(fig)

    # ---: Category Proportions ---
    try:
        fig, ax = plt.subplots(figsize=(10, 6))
        true_props = pd.Series(y_true_ordinal).value_counts(normalize=True).sort_index()
        pred_props = pd.Series(y_pred_ordinal).value_counts(normalize=True).sort_index()

        # Combine into a DataFrame for easy plotting, reindexing to include all categories
        df_props = pd.DataFrame({'Observed': true_props, 'Predicted': pred_props}).reindex(labels_sorted).fillna(0)
        df_props.plot(kind='bar', ax=ax, width=0.8, color=['#1f77b4', '#ff7f0e'])

        ax.set_title(
            f"Observed vs. Predicted Outcome Proportions (Test Set)\n"
            f"{model_info['model_name']} | {model_info['fs_name']} @ {model_info['h']}h",
            fontsize=14
        )
        ax.set_ylabel("Proportion of Patients", fontsize=12)
        ax.set_xlabel("GOSE Outcome Category", fontsize=12)
        ax.set_xticklabels(display_labels, rotation=45, ha='right')
        ax.legend(title="Distribution")
        ax.grid(axis='y', linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.savefig(os.path.join(model_folder, "final_category_proportions.png"), dpi=300)
        plt.close(fig)
    except Exception as e:
        logging.warning(f"Could not generate Category Proportion plot: {e}")


    # --- SHAP Analysis (if available) ---
    if SHAP_AVAILABLE:
        try:
            logging.info(f"  Generating SHAP analysis for {model_info['model_name']} model...")
            preprocessor = final_pipeline.named_steps['preprocessor']
            classifier = final_pipeline.named_steps['classifier']

            X_test_transformed = preprocessor.transform(X_test_eval)
            try:
                feature_names = preprocessor.get_feature_names_out()
            except AttributeError:
                feature_names = [] # Fallback logic is already in the script
                for name, trans, cols in preprocessor.transformers_:
                    if trans == 'drop' or not cols: continue
                    if name == 'num' and isinstance(trans, Pipeline):
                        if 'indicator' in trans.named_steps and trans.named_steps['indicator']:
                            indicator_indices = trans.named_steps['indicator'].indicator_.features_
                            indicator_names = [f"missing_{cols[i]}" for i in indicator_indices]
                            feature_names.extend(cols); feature_names.extend(indicator_names)
                        else: feature_names.extend(cols)
                    elif name == 'cat' and isinstance(trans, Pipeline) and 'ohe' in trans.named_steps:
                        ohe = trans.named_steps['ohe']
                        feature_names.extend(ohe.get_feature_names_out(cols))
                    else: feature_names.extend(cols)

            X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test_eval.index)
            if len(X_test_transformed_df) == 0:
                logging.warning("  No test rows available for SHAP; skipping.")
            else:
                sample_size = min(len(X_test_transformed_df), cfg["ARTIFACTS"]["SHAP_SAMPLE_SIZE"])
                X_test_sample = X_test_transformed_df.sample(sample_size, random_state=cfg["MODELING"]["RANDOM_STATE"])

                X_train_transformed = preprocessor.transform(X_train)
                X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)
                X_train_sample_bg = X_train_transformed_df.sample(min(len(X_train_transformed_df), 200), random_state=cfg["MODELING"]["RANDOM_STATE"])

                classifier = final_pipeline.named_steps['classifier']

                if model_info['model_name'] == MODEL_XGB:
                    explainer = shap.Explainer(classifier, X_train_sample_bg)
                    shap_values_all_classes = explainer(X_test_sample)
                    shap_values_single_class = shap_values_all_classes[:, :, -1]
                
                elif model_info['model_name'] == MODEL_ORDINAL:
                    # Use the classifier directly on transformed data
                    explainer = shap.Explainer(classifier, X_train_sample_bg)
                    shap_values_all_classes = explainer(X_test_sample)
                    shap_values_single_class = shap_values_all_classes[:, :, -1]
                    
                if explainer:
                    # Helper for consistent plotting
                    def _save_shap_plot(plot_type, shap_vals, suffix, title):
                        plt.figure()
                        shap.summary_plot(shap_vals, X_test_sample, plot_type=plot_type, show=False, max_display=40)
                        fig = plt.gcf(); fig.set_size_inches(10, 15)
                        plt.title(title, fontsize=14); plt.tight_layout()
                        plt.savefig(os.path.join(model_folder, f'shap_summary_{suffix}.png'), dpi=300)
                        plt.close(fig)
                    
                    base_title = f"{model_info['model_name']} | {model_info['fs_name']} @ {model_info['h']}h"
                    
                    # Bar plot for overall importance (can use multi-output SHAP values)
                    _save_shap_plot(
                        "bar", 
                        shap_values_all_classes, 
                        "bar_importance", 
                        f"SHAP Feature Importance (Mean Absolute Value)\n{base_title}"
                    )
                    
                    # Beeswarm plot for feature impact (MUST use single-output SHAP values)
                    _save_shap_plot(
                        "dot", 
                        shap_values_single_class, 
                        "beeswarm_impact", 
                        f"SHAP Feature Impact on Prediction for Favorable Outcome\n{base_title}" # More descriptive title
                    )

        except Exception as e:
            logging.warning(f"  SHAP generation failed: {e}", exc_info=True)

    # --- Export training feature list ---
    try:
        pd.DataFrame({'feature_name': X_train.columns}).to_csv(
            os.path.join(model_folder, "train_features.csv"),
            index=False
        )
        logging.info(f"  Exported {len(X_train.columns)} feature names to train_features.csv")
    except Exception as e:
        logging.warning(f"Could not export feature list: {e}")

# ============================== MAIN EXECUTION ================================
"""
    This is the main conductor of the entire analysis pipeline. It orchestrates all the steps from
    loading the data to generating the final summary tables, ensuring a rigorous and reproducible workflow.

    The key steps it performs are:
    1. Loads the master patient database and splits it into a training set (older data) and a test set (newer data).
    2. Repeats the entire modeling process at different time points after injury (e.g., 24h, 48h), which we call 'landmarks'.
    3. For each landmark, it correctly identifies the group of patients who were still alive and at risk.
    4. It then generates all the necessary predictor variables, from simple baseline data to the complex dynamic features from ICU monitoring.
    5. It defines two important patient groups for comparison:
        a) all_at_risk: All patients alive at the landmark, used to assess the baseline model's overall performance.
        b) icu_only_head_to_head: A specific subgroup of patients who have ICU data, used for a fair, direct comparison between the baseline and advanced ICU models.
    6. Within these groups, it trains, tunes, and rigorously evaluates the different models.
    7. Finally, it compiles all the results into clear summary tables that compare the performance of all the models.
"""
def main():
    enable_quiet_mode(True)
    start_time = time.time(); setup_environment(CONFIG)

    # This prevents accidental mismatches between the initial data split and the CV method.
    split_strategy = CONFIG["DATA_PROCESSING"].get("SPLIT_STRATEGY", "temporal")
    cv_type = CONFIG["MODELING"].get("CV_TYPE", "timeseries")
    if (split_strategy == "temporal" and cv_type != "timeseries") or \
       (split_strategy == "stratified" and cv_type != "stratified"):
        raise ValueError(
            f"Configuration mismatch! SPLIT_STRATEGY ('{split_strategy}') and "
            f"CV_TYPE ('{cv_type}') must be aligned (temporal/timeseries or stratified/stratified)."
        )

    features_manifest = []

    try:
        # load_and_prepare_master_data now correctly splits only on patients with a valid trauma time.
        (master_train, master_test), raw_baseline_feats, flow_log_from_prep = load_and_prepare_master_data(CONFIG)
        
        y_train_all = master_train[[COL_TARGET_ORDINAL_0_6, COL_TARGET_MORTALITY, COL_TARGET_FAVORABLE, COL_TARGET_GOSE_1_8]]
        y_test_all = master_test[[COL_TARGET_ORDINAL_0_6, COL_TARGET_MORTALITY, COL_TARGET_FAVORABLE, COL_TARGET_GOSE_1_8]]
        all_results = []
        cohort_data_for_tables = {}

        for h in CONFIG["FEATURE_ENGINEERING"]["PREDICTION_LANDMARKS_HOURS"]:
            logging.info(f"\n{'='*25} PREDICTION LANDMARK: {h}h {'='*25}")
            
            # ---  RIGOROUS AND CORRECT COHORT DEFINITION FOR LANDMARK 'h' ---
            # This block  ensure thebbaseline "all at risk" cohort is correctly defined.

            # 1. Start with the full train/test sets from the initial split.
            master_train_h_potential = master_train.copy()
            master_test_h_potential = master_test.copy()

            # 2. Define the "at risk" cohort: those who were alive at landmark 'h'.
            death_flag = f'Died_within_{h}h'
            # The .fillna(True) conservatively excludes any patient with an ambiguous death status from the risk set.
            train_pids_at_risk = master_train_h_potential.index[~master_train_h_potential[death_flag].fillna(True)]
            test_pids_at_risk = master_test_h_potential.index[~master_test_h_potential[death_flag].fillna(True)]
            
            if len(train_pids_at_risk) < 50:
                logging.warning(f"Train risk set for {h}h is too small ({len(train_pids_at_risk)}). Skipping landmark.")
                continue

            # 3. Create the definitive "at risk" dataframes for this landmark. ALL subsequent
            #    feature sets will be derived from these consistent cohorts.
            master_train_h = master_train_h_potential.loc[train_pids_at_risk]
            master_test_h = master_test_h_potential.loc[test_pids_at_risk]
            
            logging.info(f"Correct Risk Set @ {h}h: {len(master_train_h)} train, {len(master_test_h)} test patients.")

            # 4. Now, build ALL feature sets from these consistent "at risk" dataframes.
            baseline_train_all = build_baseline_for_landmark(master_train_h, raw_baseline_feats, h)
            baseline_test_all = build_baseline_for_landmark(master_test_h, raw_baseline_feats, h)
            
            icu_train = generate_dynamic_features_for_landmark(master_train_h, CONFIG["FILE_PATHS"]["ICU_DATA_FOLDER"], h, CONFIG)
            icu_test = generate_dynamic_features_for_landmark(master_test_h, CONFIG["FILE_PATHS"]["ICU_DATA_FOLDER"], h, CONFIG)


            features_for_sanity_check = ['dyn_ICPm_mean', 'dyn_CPP_mean', 'meta_has_ICPm', 'meta_has_CPP', 'dose_ICPm_auc_gt_20', 'dose_CPP_auc_lt_60']
            perform_sanity_check_on_features(dynamic_features_df=icu_train, y_data=y_train_all, features_to_check=features_for_sanity_check, group_name=f"{h}h Training Set")

            # === Cohort-level ICU event audit ===
            try:
                audit_event_burdens_and_episodes(icu_train, h, CONFIG, split_label="train")
                audit_event_burdens_and_episodes(icu_test,  h, CONFIG, split_label="test")
                if not icu_train.empty or not icu_test.empty:
                    icu_combined = pd.concat([icu_train, icu_test], axis=0, copy=False)
                    audit_event_burdens_and_episodes(icu_combined, h, CONFIG, split_label="combined")
            except Exception as e:
                logging.warning(f"[AUDIT] Event/episode audit failed for {h}h: {e}", exc_info=False)

            pids_train_all = train_pids_at_risk
            pids_test_all = test_pids_at_risk
            pids_train_icu = icu_train.index
            pids_test_icu = icu_test.index
            logging.info(f"ICU Cohort @ {h}h: Found {len(pids_train_icu)} train and {len(pids_test_icu)} test patients with ICU data.")
            cohort_data_for_tables[f"at_risk_{h}h_ids"] = set(train_pids_at_risk).union(set(test_pids_at_risk))
            cohort_data_for_tables[f"h2h_{h}h_ids"] = set(pids_train_icu).union(set(pids_test_icu))
            analysis_cohorts = {"all_at_risk": (pids_train_all, pids_test_all), "icu_only_head_to_head": (pids_train_icu, pids_test_icu)}

            for group_name, (train_pids, test_pids) in analysis_cohorts.items():
                logging.info(f"\n--- Running Analysis Group: '{group_name}' for {h}h landmark ---")
                if len(train_pids) == 0:
                    logging.warning(f"Skipping group '{group_name}' as it has no training patients.")
                    continue
                feature_sets_to_run = {}
                feature_sets_to_run["baseline_only"] = (baseline_train_all.loc[train_pids], baseline_test_all.loc[test_pids])
                if group_name == "icu_only_head_to_head":
                    combined_train = baseline_train_all.loc[train_pids].join(icu_train.loc[train_pids], how='inner')
                    combined_test = baseline_test_all.loc[test_pids].join(icu_test.loc[test_pids], how='inner')
                    feature_sets_to_run["baseline_plus_icu"] = (combined_train, combined_test)
                # === ICU ablation subsets ===
                try:
                    subsets = build_icu_feature_subsets(icu_train)
                    for sub_name, sub_cols in subsets.items():
                        if not sub_cols:
                            continue
                        train_sub = icu_train.loc[train_pids, sub_cols].dropna(how='all', axis=1)
                        test_sub = icu_test.loc[test_pids, sub_cols].dropna(how='all', axis=1)
                        if train_sub.shape[1] == 0:
                            continue
                        combined_train_sub = baseline_train_all.loc[train_pids].join(train_sub, how='inner')
                        combined_test_sub = baseline_test_all.loc[test_pids].join(test_sub, how='inner')
                        feature_sets_to_run[f"baseline_plus_{sub_name}"] = (combined_train_sub, combined_test_sub)
                except Exception as e:
                    logging.debug(f"Could not build ICU ablation subsets: {e}")


                for fs_name, (X_train_raw, X_test_raw) in feature_sets_to_run.items():
                    if X_train_raw.empty:
                        logging.warning(f"Feature set '{fs_name}' is empty for training in group '{group_name}'. Skipping.")
                        continue
                    if fs_name.startswith("baseline_plus") and CONFIG["FEATURE_SELECTION"]["ENABLE_ICU_SELECTION"]:
                        baseline_cols = [c for c in X_train_raw.columns if c in baseline_train_all.columns]
                        icu_cols = [c for c in X_train_raw.columns if c in icu_train.columns]
                        X_baseline_train_fs = X_train_raw[baseline_cols]
                        X_icu_train_fs = X_train_raw[icu_cols]
                        y_train_fs = y_train_all.loc[X_train_raw.index, COL_TARGET_ORDINAL_0_6]
                        selected_features = select_features_for_icu_model(X_baseline_train_fs, X_icu_train_fs, y_train_fs, CONFIG)
                        X_train_raw = X_train_raw[selected_features]
                        X_test_raw = X_test_raw.reindex(columns=selected_features, fill_value=np.nan)

                    X_train = drop_sparse_columns(X_train_raw, min_frac=0.25)
                    final_cols = X_train.columns.tolist()
                    X_test = X_test_raw.reindex(columns=final_cols, fill_value=np.nan)
                    y_train = y_train_all.loc[X_train.index]
                    y_test = y_test_all.loc[X_test.index]
                    n_test_effective = y_test[COL_TARGET_ORDINAL_0_6].notna().sum()

                    baseline_pool = set(baseline_train_all.columns)
                    used_baseline = [c for c in final_cols if c in baseline_pool]
                    used_icu      = [c for c in final_cols if c not in baseline_pool]
                    features_manifest.append({
                        'landmark_h': h,
                        'comparison_group': group_name,
                        'feature_set': fs_name,
                        'n_features_total': len(final_cols),
                        'n_features_baseline': len(used_baseline),
                        'n_features_icu': len(used_icu),
                        'features_baseline': ';'.join(used_baseline),
                        'features_icu': ';'.join(used_icu),
                    })

                    for model_name in [MODEL_XGB, MODEL_ORDINAL]:
                        if model_name == MODEL_ORDINAL and not MORD_AVAILABLE: continue
                        logging.info(f"--- Training: {model_name} | {fs_name} | Group: {group_name} ---")
                        s_trauma_dt = sanitize_name(COL_TRAUMA_DATETIME)
                        time_order_train = master_train_h.loc[X_train.index, s_trauma_dt]

                        pipeline, cv_qwk = fit_and_tune_model(
                            X_train, y_train_all.loc[X_train.index, COL_TARGET_ORDINAL_0_6],
                            model_name, CONFIG,
                            time_order=(time_order_train if CONFIG["MODELING"].get("CV_TYPE","stratified").lower()=="timeseries" else None)
                        )

                        if pipeline:
                            logging.info(f"  > Best CV Score: {cv_qwk:.4f}")
                            model_info = {'h': h, 'fs_name': fs_name, 'model_name': model_name, 'comparison_group': group_name}
                            evaluate_and_save_artifacts(pipeline, X_test, y_test, model_info, X_train.copy(), CONFIG)

                            test_mask = y_test[COL_TARGET_ORDINAL_0_6].notna()
                            test_qwk, test_acc, auroc_mortality, auroc_favorable, test_aw1, test_aw2 = (np.nan,) * 6
                            ece_mortality, cal_slope_mortality, cal_in_large_mortality = (np.nan,) * 3
                            ece_favorable, cal_slope_favorable, cal_in_large_favorable = (np.nan,) * 3

                            if test_mask.any():
                                y_true_test_ord = y_test.loc[test_mask, COL_TARGET_ORDINAL_0_6].values
                                y_pred_test_ord = predict_with_qwk_thresholds(pipeline, X_test.loc[test_mask])
                                clf_step = pipeline.named_steps['classifier']
                                test_qwk = cohen_kappa_score(y_true_test_ord, y_pred_test_ord, weights='quadratic')
                                test_acc = accuracy_score(y_true_test_ord, y_pred_test_ord)
                                test_aw1 = accuracy_within_n(y_true_test_ord, y_pred_test_ord, n=1)
                                test_aw2 = accuracy_within_n(y_true_test_ord, y_pred_test_ord, n=2)

                                if hasattr(clf_step, "predict_proba"):
                                    try:
                                        y_pred_proba_all = pipeline.predict_proba(X_test.loc[test_mask])
                                        classes = getattr(clf_step, "classes_", np.arange(y_pred_proba_all.shape[1]))
                                        if np.issubdtype(classes.dtype, np.integer) and classes.min() == 1:
                                            classes = classes - 1
                                        cls_to_idx = {int(c): i for i, c in enumerate(classes)}

                                        dead_vs_idx = cls_to_idx.get(0)
                                        if dead_vs_idx is not None:
                                            y_prob_dv = y_pred_proba_all[:, dead_vs_idx]
                                            y_true_dv = (y_test.loc[test_mask, COL_TARGET_GOSE_1_8].isin([1, 2])).astype(int).values
                                            if len(np.unique(y_true_dv)) > 1:
                                                auroc_mortality = roc_auc_score(y_true_dv, y_prob_dv)
                                                cal_metrics_dv = calculate_calibration_metrics(y_true_dv, y_prob_dv)
                                                ece_mortality = cal_metrics_dv['ece']
                                                cal_slope_mortality = cal_metrics_dv['cal_slope']
                                                cal_in_large_mortality = cal_metrics_dv['cal_in_large']

                                        fav_indices = [cls_to_idx.get(c) for c in classes if c >= 3 and cls_to_idx.get(c) is not None]
                                        if fav_indices:
                                            y_prob_fav = y_pred_proba_all[:, fav_indices].sum(axis=1)
                                            y_true_fav = y_test.loc[test_mask, COL_TARGET_FAVORABLE].values
                                            if len(np.unique(y_true_fav)) > 1:
                                                auroc_favorable = roc_auc_score(y_true_fav, y_prob_fav)
                                                cal_metrics_fav = calculate_calibration_metrics(y_true_fav, y_prob_fav)
                                                ece_favorable = cal_metrics_fav['ece']
                                                cal_slope_favorable = cal_metrics_fav['cal_slope']
                                                cal_in_large_favorable = cal_metrics_fav['cal_in_large']
                                    except Exception as e:
                                        logging.warning(f"Could not calculate AUROC/Calibration for summary table: {e}")

                            all_results.append({
                                'landmark_h': h, 'comparison_group': group_name, 'feature_set': fs_name,
                                'model': model_name, 'cv_score': cv_qwk, 'test_qwk': test_qwk,
                                'test_accuracy': test_acc, 'test_aw1': test_aw1, 'test_aw2': test_aw2,
                                'auroc_mortality': auroc_mortality, 'auroc_favorable': auroc_favorable,
                                'ece_mortality': ece_mortality, 'cal_slope_mortality': cal_slope_mortality,
                                'ece_favorable': ece_favorable, 'cal_slope_favorable': cal_slope_favorable,
                                'n_train': len(X_train), 'n_test': n_test_effective, 'n_features': X_train.shape[1],
                            })

        if not all_results: raise RuntimeError("No models were successfully trained.")
        df_summary = pd.DataFrame(all_results)
        logging.info("\n\n" + "="*30 + " FINAL RESULTS SUMMARY " + "="*30)
        df_head_to_head = df_summary[df_summary['comparison_group'] == 'icu_only_head_to_head']
        if not df_head_to_head.empty:
            pivot_h2h = df_head_to_head.pivot_table(index=['landmark_h', 'model'], columns='feature_set', values='test_qwk')
            logging.info("\n--- Test QWK Head-to-Head Comparison (ICU Patient Cohort Only) ---\n" + pivot_h2h.to_string(float_format="%.3f"))
        logging.info("\n--- Full Detailed Results (Grouped by Landmark) ---\n" + df_summary.sort_values(by=["landmark_h", "comparison_group", "feature_set", "model"], ascending=[True, True, True, True]).to_string(index=False, float_format="%.3f"))
        df_summary.to_csv(os.path.join(CONFIG["RUN_INFO"]["OUTPUT_FOLDER"], "final_results_summary.csv"), index=False)
        pd.DataFrame(features_manifest).to_csv(os.path.join(CONFIG["RUN_INFO"]["OUTPUT_FOLDER"], "features_manifest.csv"), index=False)
        
        # ==== ICU ablation ΔQWK report ====
        try:
            df_icu = df_summary[df_summary['comparison_group'] == 'icu_only_head_to_head'].copy()
            base = df_icu[df_icu['feature_set'] == 'baseline_only'][['landmark_h', 'model', 'test_qwk']].rename(columns={'test_qwk':'baseline_test_qwk'})
            df_icu = df_icu.merge(base, on=['landmark_h','model'], how='left')
            df_icu['delta_vs_baseline'] = df_icu['test_qwk'] - df_icu['baseline_test_qwk']
            df_icu.to_csv(os.path.join(CONFIG["RUN_INFO"]["OUTPUT_FOLDER"], "final_results_ablation.csv"), index=False)
            piv = df_icu.pivot_table(index=['landmark_h','model'], columns='feature_set', values='delta_vs_baseline')
            logging.info("\n--- ΔQWK vs Baseline (ICU head-to-head) ---\n" + piv.to_string(float_format="%.3f"))
        except Exception as e:
            logging.warning(f"Could not write ablation ΔQWK report: {e}")
        
        try:
            generate_publication_tables(
                cfg=CONFIG,
                cohort_data=cohort_data_for_tables,
                master_train_df=master_train,
                master_test_df=master_test,
                flow_log=flow_log_from_prep
            )
        except Exception as e:
            logging.error(f"An error occurred during final table generation: {e}", exc_info=True)

    except Exception as e:
        logging.exception("An error occurred during the main pipeline execution.")
        sys.exit(1)
    finally:
        logging.info(f"\n--- Pipeline finished in {(time.time() - start_time) / 60:.2f} minutes ---")

if __name__ == "__main__":
    main()
